{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5rmpybwysXGV"
   },
   "source": [
    "### osu!nn #7: GAN map flow generator\n",
    "\n",
    "Generate a nice map using a GAN and the data we have gathered until now.\n",
    "\n",
    "Synthesis of \"flowData\"\n",
    "* training_flowData x 10 ~ 99 (Quality: 60+)\n",
    "* rhythmData x 1\n",
    "* momentumData x 1\n",
    "* (Discriminator) x 1\n",
    "* (Generator) x 1\n",
    "\n",
    "Synthesis Time: ~15 mins\n",
    "\n",
    "Last edit: 2019/4/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "m8y3rGtQsYP2"
   },
   "source": [
    "#### First of all, let's welcome -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "hrXv0rU9sIma"
   },
   "source": [
    "## Cute Sophie!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "7S0BwJ_8sLu7"
   },
   "source": [
    "<img src=\"https://i.imgur.com/Ko2wogO.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "k2o3TTG4TFpt"
   },
   "source": [
    "In the previous notebook we have predicted... or estimated our rhythm, and now we will be trying to create the new map by imitating the existing dataset, using a Generative Adversial Network (GAN).\n",
    "\n",
    "Note that this GAN is irrelevant to the music, not the time interval, only the coordinates of notes themselves.\n",
    "\n",
    "Probably could get some slider coordinates inbetween? this way it may learn something about slider shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "PJ64L90aVir3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re, subprocess, json\n",
    "from datetime import datetime\n",
    "\n",
    "# Tensorflow 2.0 enables eager automatically\n",
    "try:\n",
    "    tf.enable_eager_execution();\n",
    "    tfe = tf.contrib.eager;\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some parameters related to GAN training.\n",
    "\n",
    "May or may not affect the result; haven't experimented a lot about these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GAN_PARAMS = {\n",
    "    \"divisor\" : 4,\n",
    "    \"good_epoch\" : 6,\n",
    "    \"max_epoch\" : 25,\n",
    "    \"note_group_size\" : 10,\n",
    "    \"g_epochs\" : 7,\n",
    "    \"c_epochs\" : 3,\n",
    "    \"g_batch\" : 50,\n",
    "    \"g_input_size\" : 50,\n",
    "    \"c_true_batch\" : 50,\n",
    "    \"c_false_batch\" : 10,\n",
    "    \"slider_max_ticks\" : 8,\n",
    "    \"next_from_slider_end\" : False\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "hBkaqxc2TsrW"
   },
   "source": [
    "#### Import the rhythm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        objs = data[\"objs\"];\n",
    "        obj_indices = [i for i,k in enumerate(objs) if k == 1];\n",
    "        predictions = data[\"predictions\"];\n",
    "        momenta = data[\"momenta\"];\n",
    "        ticks = data[\"ticks\"];\n",
    "        timestamps = data[\"timestamps\"];\n",
    "        sv = data[\"sv\"];\n",
    "        dist_multiplier = data[\"dist_multiplier\"];\n",
    "    return objs, obj_indices, predictions, momenta, ticks, timestamps, sv, dist_multiplier;\n",
    "\n",
    "unfiltered_objs, obj_indices, unfiltered_predictions, unfiltered_momenta, unfiltered_ticks, unfiltered_timestamps, unfiltered_sv, dist_multiplier = read_npz(\"rhythm_data.npz\");\n",
    "\n",
    "first_step_objs =        unfiltered_objs[obj_indices];\n",
    "first_step_predictions = unfiltered_predictions[obj_indices];\n",
    "first_step_momenta =     unfiltered_momenta[obj_indices];\n",
    "first_step_ticks =       unfiltered_ticks[obj_indices];\n",
    "first_step_timestamps =  unfiltered_timestamps[obj_indices];\n",
    "first_step_sv =          unfiltered_sv[obj_indices];\n",
    "\n",
    "momentum_multiplier = 1.0;\n",
    "angular_momentum_multiplier = 1.0;\n",
    "\n",
    "first_step_is_slider = first_step_predictions[:, 2];\n",
    "first_step_is_spinner = first_step_predictions[:, 3];\n",
    "first_step_is_sliding = first_step_predictions[:, 4];\n",
    "first_step_is_spinning = first_step_predictions[:, 5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert notes with is_slider flag to sliders\n",
    "# if there is next note, slide to next note\n",
    "# else, slide for 4 ticks\n",
    "\n",
    "# Problems:\n",
    "# - how to deal with momenta? (change the slider speed! which is obviously not good)\n",
    "# - do we use AM?\n",
    "# - do we use is_sliding?\n",
    "# - do we use the slider model? it's heavily overfit... (try with some other dataset, other than sota!!!)\n",
    "# - does the shape cause overlapping? add some penalty loss? let it learn from classifier? ...\n",
    "# - and many more!\n",
    "skip_this = False;\n",
    "new_obj_indices = [];\n",
    "slider_ticks = [];\n",
    "slider_max_ticks = GAN_PARAMS[\"slider_max_ticks\"];\n",
    "for i in range(len(first_step_objs)):\n",
    "    if skip_this:\n",
    "        first_step_is_slider[i] = 0;\n",
    "        skip_this = False;\n",
    "        continue;\n",
    "    if first_step_is_slider[i]: # this one is a slider!!\n",
    "        if i == first_step_objs.shape[0]-1: # Last Note.\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "            continue;\n",
    "        if first_step_ticks[i+1] >= first_step_ticks[i] + slider_max_ticks + 1: # too long! end here\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(slider_max_ticks);\n",
    "        else:\n",
    "            skip_this = True;\n",
    "            new_obj_indices.append(i);\n",
    "            slider_ticks.append(max(1, first_step_ticks[i+1] - first_step_ticks[i]));\n",
    "    else: # not a slider!\n",
    "        new_obj_indices.append(i);\n",
    "        slider_ticks.append(0);\n",
    "\n",
    "# Filter the removed objects out!\n",
    "objs =        first_step_objs[new_obj_indices];\n",
    "predictions = first_step_predictions[new_obj_indices];\n",
    "momenta =     first_step_momenta[new_obj_indices];\n",
    "ticks =       first_step_ticks[new_obj_indices];\n",
    "timestamps =  first_step_timestamps[new_obj_indices];\n",
    "is_slider =   first_step_is_slider[new_obj_indices];\n",
    "is_spinner =  first_step_is_spinner[new_obj_indices];\n",
    "is_sliding =  first_step_is_sliding[new_obj_indices];\n",
    "is_spinning = first_step_is_spinning[new_obj_indices];\n",
    "sv =          first_step_sv[new_obj_indices];\n",
    "slider_ticks = np.array(slider_ticks);\n",
    "\n",
    "# get divisor from GAN_PARAMS\n",
    "divisor = GAN_PARAMS[\"divisor\"];\n",
    "\n",
    "# should be slider length each tick, which is usually SV * SMP * 100 / 4\n",
    "# e.g. SV 1.6, timing section x1.00, 1/4 divisor, then slider_length_base = 40\n",
    "slider_length_base = sv // divisor;\n",
    "\n",
    "# these data must be kept consistent with the sliderTypes in load_map.js\n",
    "slider_types = np.random.randint(0, 5, is_slider.shape).astype(int); # needs to determine the slider types!! also it is 5!!!\n",
    "slider_type_rotation = np.array([0, -0.40703540572409336, 0.40703540572409336, -0.20131710837464062, 0.20131710837464062]);\n",
    "slider_cos = np.cos(slider_type_rotation);\n",
    "slider_sin = np.sin(slider_type_rotation);\n",
    "\n",
    "slider_cos_each = slider_cos[slider_types];\n",
    "slider_sin_each = slider_sin[slider_types];\n",
    "\n",
    "# this is vector length! I should change the variable name probably...\n",
    "slider_type_length = np.array([1.0, 0.97, 0.97, 0.97, 0.97]);\n",
    "\n",
    "slider_lengths = np.array([slider_type_length[int(k)] * slider_length_base[i] for i, k in enumerate(slider_types)]) * slider_ticks;\n",
    "\n",
    "# print(slider_lengths.shape)\n",
    "# print(timestamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For distances starting from slider ends\n",
    "tick_lengths_pre = (timestamps[1:] - timestamps[:-1]) / (ticks[1:] - ticks[:-1]);\n",
    "tick_lengths = np.concatenate([tick_lengths_pre, [tick_lengths_pre[-1]]]);\n",
    "timestamps_note_end = timestamps + slider_ticks * tick_lengths;\n",
    "\n",
    "timestamps_plus_1 = np.concatenate([timestamps[1:], timestamps[-1:] + (timestamps[-1:] - timestamps[-2:-1])])\n",
    "\n",
    "if GAN_PARAMS[\"next_from_slider_end\"]:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps_note_end;\n",
    "    timestamps_before = np.concatenate([[6662], timestamps_after[:-1]]); # why 6662????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "else:\n",
    "    timestamps_after = timestamps_plus_1 - timestamps;\n",
    "    timestamps_before = np.concatenate([[4777], timestamps_after[:-1]]); # why 4777????\n",
    "    note_distances = timestamps_before * momenta[:, 0] * momentum_multiplier;\n",
    "note_angles = timestamps_before * momenta[:, 1] * angular_momentum_multiplier;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "is_slider = predictions[:, 2];\n",
    "is_sliding = predictions[:, 4];\n",
    "#print(is_slider * is_sliding - is_slider); # is all 0!!\n",
    "# print(is_slider * is_sliding);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plotting functions (only for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "VAkAvc3ATskA"
   },
   "outputs": [],
   "source": [
    "from plthelper import MyLine, plot_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the data for classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "chunk_size = GAN_PARAMS[\"note_group_size\"];\n",
    "step_size = 5;\n",
    "\n",
    "max_x = 512;\n",
    "max_y = 384;\n",
    "\n",
    "# \"TICK\", \"TIME\", \"TYPE\", \"X\", \"Y\", \"IN_DX\", \"IN_DY\", \"OUT_DX\", \"OUT_DY\"\n",
    "def read_map_npz(file_path):\n",
    "    with np.load(file_path) as data:\n",
    "        flow_data = data[\"flow\"];\n",
    "    return flow_data;\n",
    "\n",
    "# TICK, TIME, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY\n",
    "def read_maps():\n",
    "    result = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            #print(os.path.join(root, file));\n",
    "            flow_data = read_map_npz(os.path.join(root, file));\n",
    "            for i in range(0, (flow_data.shape[0] - chunk_size) // step_size):\n",
    "                chunk = flow_data[i * step_size:i * step_size + chunk_size];\n",
    "                result.append(chunk);\n",
    "                \n",
    "    # normalize the TICK col and remove TIME col\n",
    "    result = np.array(result)[:, :, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10]];\n",
    "    result[:, :, 0] %= divisor;\n",
    "    result[:, :, 2] /= max_x;\n",
    "    result[:, :, 3] /= max_y;\n",
    "    result[:, :, 8] /= max_x;\n",
    "    result[:, :, 9] /= max_y;\n",
    "    \n",
    "    # TICK, TYPE, X, Y, IN_DX, IN_DY, OUT_DX, OUT_DY, END_X, END_Y\n",
    "    # only use X,Y,OUT_DX,OUT_DY,END_X,END_Y\n",
    "    result = np.array(result)[:, :, [2, 3, 6, 7, 8, 9]];\n",
    "    return result;\n",
    "\n",
    "# The default dataset so people don't have to come up with a whole dataset to use this.\n",
    "# To save the flow data to a flow_dataset.npz, it is simple - just run the following after reading maps:\n",
    "# np.savez_compressed(\"flow_dataset\", maps = maps);\n",
    "try:\n",
    "    maps = read_maps();\n",
    "    labels = np.ones(maps.shape[0]);\n",
    "except:\n",
    "    with np.load(\"flow_dataset.npz\") as flow_dataset:\n",
    "        maps = flow_dataset[\"maps\"];\n",
    "        labels = np.ones(maps.shape[0]);\n",
    "\n",
    "order2 = np.argsort(np.random.random(maps.shape[0]));\n",
    "special_train_data = maps[order2];\n",
    "special_train_labels = labels[order2];\n",
    "# order3 = np.argsort(np.random.random(false_maps.shape[0]));\n",
    "# special_false_data = false_maps[order2];\n",
    "# special_false_labels = false_labels[order2];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define the classifier model.\n",
    "\n",
    "The model structure can be probably optimized... while I currently have no good idea about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras;\n",
    "\n",
    "def build_classifier_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape=(special_train_data.shape[1], special_train_data.shape[2])),\n",
    "        keras.layers.Dense(64),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1, activation=tf.nn.tanh),\n",
    "        keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(1,)),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "qutT_fkl_CBc"
   },
   "source": [
    "Functions for map generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "gxPTb-kt_N5m"
   },
   "outputs": [],
   "source": [
    "from tfhelper import *\n",
    "\n",
    "# A regularizer to keep the map inside the box.\n",
    "# It's so the sliders and notes don't randomly fly out of the screen!\n",
    "def inblock_loss(vg):\n",
    "    wall_var_l = tf.where(tf.less(vg, 0.2), tf.square(0.3 - vg), 0 * vg);\n",
    "    wall_var_r = tf.where(tf.greater(vg, 0.8), tf.square(vg - 0.7), 0 * vg);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def inblock_trueness(vg):\n",
    "    wall_var_l = tf.cast(tf.less(vg, 0), tf.float32);\n",
    "    wall_var_r = tf.cast(tf.greater(vg, 1), tf.float32);\n",
    "    return tf.reduce_mean(tf.reduce_mean(wall_var_l + wall_var_r, axis=2), axis=1);\n",
    "\n",
    "def cut_map_chunks(c):\n",
    "    r = [];\n",
    "    for i in range(0, (c.shape[0] - chunk_size) // step_size):\n",
    "        chunk = c[i * step_size:i * step_size + chunk_size];\n",
    "        r.append(chunk);\n",
    "    return tf.stack(r);\n",
    "\n",
    "def construct_map_with_sliders(var_tensor, extvar=[]):\n",
    "    var_tensor = tf.cast(var_tensor, tf.float32);\n",
    "    var_shape = var_tensor.shape;\n",
    "    wall_l = 0.15;\n",
    "    wall_r = 0.85;\n",
    "    x_max = 512;\n",
    "    y_max = 384;\n",
    "    out = [];\n",
    "    cp = tf.constant([256, 192, 0, 0]);\n",
    "    phase = 0;\n",
    "    half_tensor = var_shape[1]//4;\n",
    "    \n",
    "    # length multiplier\n",
    "    if \"length_multiplier\" in extvar:\n",
    "        length_multiplier = extvar[\"length_multiplier\"];\n",
    "    else:\n",
    "        length_multiplier = 1;\n",
    "\n",
    "    # notedists\n",
    "    if \"begin\" in extvar:\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "    else:\n",
    "        begin_offset = 0;\n",
    "    \n",
    "#     note_distances_now = length_multiplier * np.expand_dims(note_distances[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "#     note_angles_now = np.expand_dims(note_angles[begin_offset:begin_offset+half_tensor], axis=0);\n",
    "    \n",
    "    relevant_tensors = extvar[\"relevant_tensors\"];\n",
    "    relevant_is_slider =      relevant_tensors[\"is_slider\"];\n",
    "    relevant_slider_lengths = relevant_tensors[\"slider_lengths\"];\n",
    "    relevant_slider_types =   relevant_tensors[\"slider_types\"];\n",
    "    relevant_slider_cos =     relevant_tensors[\"slider_cos_each\"];\n",
    "    relevant_slider_sin =     relevant_tensors[\"slider_sin_each\"];\n",
    "    relevant_note_distances = relevant_tensors[\"note_distances\"];\n",
    "    relevant_note_angles =    relevant_tensors[\"note_angles\"];\n",
    "    \n",
    "    note_distances_now = length_multiplier * tf.expand_dims(relevant_note_distances, axis=0);\n",
    "    note_angles_now = tf.expand_dims(relevant_note_angles, axis=0);\n",
    "\n",
    "    # init\n",
    "    l = tf.convert_to_tensor(note_distances_now, dtype=\"float32\");\n",
    "    sl = l * 0.7;\n",
    "    sr = tf.convert_to_tensor(note_angles_now, dtype=\"float32\");\n",
    "    \n",
    "    cos_list = var_tensor[:, 0:half_tensor * 2];\n",
    "    sin_list = var_tensor[:, half_tensor * 2:];\n",
    "    len_list = tf.sqrt(tf.square(cos_list) + tf.square(sin_list));\n",
    "    cos_list = cos_list / len_list;\n",
    "    sin_list = sin_list / len_list;\n",
    "    \n",
    "    wall_l = 0.05 * x_max + l * 0.5;\n",
    "    wall_r = 0.95 * x_max - l * 0.5;\n",
    "    wall_t = 0.05 * y_max + l * 0.5;\n",
    "    wall_b = 0.95 * y_max - l * 0.5;\n",
    "    rerand = tf.cast(tf.greater(l, y_max / 2), tf.float32);\n",
    "    not_rerand = tf.cast(tf.less_equal(l, y_max / 2), tf.float32);\n",
    "    \n",
    "    next_from_slider_end = extvar[\"next_from_slider_end\"];\n",
    "\n",
    "    # generate\n",
    "    if \"start_pos\" in extvar:\n",
    "        _pre_px = extvar[\"start_pos\"][0];\n",
    "        _pre_py = extvar[\"start_pos\"][1];\n",
    "        _px = tf.cast(_pre_px, tf.float32);\n",
    "        _py = tf.cast(_pre_py, tf.float32);\n",
    "    else:\n",
    "        _px = tf.cast(256, tf.float32);\n",
    "        _py = tf.cast(192, tf.float32);\n",
    "    \n",
    "    # this is not important since the first position starts at _ppos + Δpos\n",
    "    _x = tf.cast(256, tf.float32);\n",
    "    _y = tf.cast(192, tf.float32);\n",
    "    \n",
    "    outputs = tf.TensorArray(tf.float32, half_tensor)\n",
    "\n",
    "    for k in range(half_tensor):\n",
    "        # r_max = 192, r = 192 * k, theta = k * 10\n",
    "        rerand_x = 256 + 256 * var_tensor[:, k];\n",
    "        rerand_y = 192 + 192 * var_tensor[:, k + half_tensor*2];\n",
    "\n",
    "        delta_value_x = l[:, k] * cos_list[:, k];\n",
    "        delta_value_y = l[:, k] * sin_list[:, k];\n",
    "\n",
    "        # It is tensor calculation batched 8~32 each call, so if/else do not work here.\n",
    "        wall_value_l =    tf.cast(tf.less(_px, wall_l[:, k]), tf.float32);\n",
    "        wall_value_r =    tf.cast(tf.greater(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_xmid = tf.cast(tf.greater(_px, wall_l[:, k]), tf.float32) * tf.cast(tf.less(_px, wall_r[:, k]), tf.float32);\n",
    "        wall_value_t =    tf.cast(tf.less(_py, wall_t[:, k]), tf.float32);\n",
    "        wall_value_b =    tf.cast(tf.greater(_py, wall_b[:, k]), tf.float32);\n",
    "        wall_value_ymid = tf.cast(tf.greater(_py, wall_t[:, k]), tf.float32) * tf.cast(tf.less(_py, wall_b[:, k]), tf.float32);\n",
    "\n",
    "        x_delta = tf.abs(delta_value_x) * wall_value_l - tf.abs(delta_value_x) * wall_value_r + delta_value_x * wall_value_xmid;\n",
    "        y_delta = tf.abs(delta_value_y) * wall_value_t - tf.abs(delta_value_y) * wall_value_b + delta_value_y * wall_value_ymid;\n",
    "\n",
    "        _x = rerand[:, k] * rerand_x + not_rerand[:, k] * (_px + x_delta);\n",
    "        _y = rerand[:, k] * rerand_y + not_rerand[:, k] * (_py + y_delta);\n",
    "#         _x = _px + x_delta;\n",
    "#         _y = _py + y_delta;\n",
    "        \n",
    "        # calculate output vector\n",
    "        \n",
    "        # slider part\n",
    "        sln = relevant_slider_lengths[k];\n",
    "        slider_type = relevant_slider_types[k];\n",
    "        scos = relevant_slider_cos[k];\n",
    "        ssin = relevant_slider_sin[k];\n",
    "        _a = cos_list[:, k + half_tensor];\n",
    "        _b = sin_list[:, k + half_tensor];\n",
    "        # cos(a+θ) = cosa cosθ - sina sinθ\n",
    "        # sin(a+θ) = cosa sinθ + sina cosθ\n",
    "        _oa = _a * scos - _b * ssin;\n",
    "        _ob = _a * ssin + _b * scos;\n",
    "        cp_slider = tf.transpose(tf.stack([_x / x_max, _y / y_max, _oa, _ob, (_x + _a * sln) / x_max, (_y + _b * sln) / y_max]));\n",
    "        _px_slider = tf.cond(next_from_slider_end, lambda: _x + _a * sln, lambda: _x);\n",
    "        _py_slider = tf.cond(next_from_slider_end, lambda: _y + _b * sln, lambda: _y);\n",
    "        \n",
    "        # circle part\n",
    "        _a = rerand[:, k] * cos_list[:, k + half_tensor] + not_rerand[:, k] * cos_list[:, k];\n",
    "        _b = rerand[:, k] * sin_list[:, k + half_tensor] + not_rerand[:, k] * sin_list[:, k];\n",
    "        cp_circle = tf.transpose(tf.stack([_x / x_max, _y / y_max, _a, _b, _x / x_max, _y / y_max]));\n",
    "        _px_circle = _x;\n",
    "        _py_circle = _y;\n",
    "        \n",
    "        outputs = outputs.write(k, tf.where(relevant_is_slider[k], cp_slider, cp_circle))\n",
    "        _px = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _px_slider, _px_circle)\n",
    "        _py = tf.where(tf.cast(relevant_is_slider[k], tf.bool), _py_slider, _py_circle)\n",
    "\n",
    "    return tf.transpose(outputs.stack(), [1, 0, 2]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import losses_utils\n",
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "\n",
    "# Loss functions and mapping layer, to adapt to TF 2.0\n",
    "class GenerativeCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def loss_function_for_generative_model(y_true, y_pred):\n",
    "            classification = y_pred;\n",
    "            loss1 = 1 - tf.reduce_mean(classification, axis=1);\n",
    "            return loss1;\n",
    "        \n",
    "        super(GenerativeCustomLoss, self).__init__(loss_function_for_generative_model, name=name, reduction=reduction)\n",
    "\n",
    "class BoxCustomLoss(LossFunctionWrapper):\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def box_loss(y_true, y_pred):\n",
    "            map_part = y_pred;\n",
    "            return inblock_loss(map_part[:, :, 0:2]) + inblock_loss(map_part[:, :, 4:6])\n",
    "        \n",
    "        super(BoxCustomLoss, self).__init__(box_loss, name=name, reduction=reduction)\n",
    "\n",
    "class AlwaysZeroCustomLoss(LossFunctionWrapper): # why does TF not include this! this is very important in certain situations\n",
    "    def __init__(self,\n",
    "        reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n",
    "        name='generative_custom_loss'):\n",
    "        \n",
    "        def alw_zero(y_true, y_pred):\n",
    "            return tf.convert_to_tensor(0, dtype=tf.float32);\n",
    "        \n",
    "        super(AlwaysZeroCustomLoss, self).__init__(alw_zero, name=name, reduction=reduction)\n",
    "        \n",
    "        \n",
    "class KerasCustomMappingLayer(keras.layers.Layer):\n",
    "    def __init__(self, extvar, output_shape=(special_train_data.shape[1], special_train_data.shape[2]), *args, **kwargs):\n",
    "        self.extvar = extvar\n",
    "        self._output_shape = output_shape\n",
    "        self.extvar_begin = tf.Variable(tf.convert_to_tensor(extvar[\"begin\"], dtype=tf.int32), trainable=False)\n",
    "        self.extvar_lmul =  tf.Variable(tf.convert_to_tensor([extvar[\"length_multiplier\"]], dtype=tf.float32), trainable=False)\n",
    "        self.extvar_nfse =  tf.Variable(tf.convert_to_tensor(extvar[\"next_from_slider_end\"], dtype=tf.bool), trainable=False)\n",
    "        self.note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "        \n",
    "        self.extvar_spos =  tf.Variable(tf.cast(tf.zeros((2, )), tf.float32), trainable=False)\n",
    "        self.extvar_rel =   tf.Variable(tf.cast(tf.zeros((7, self.note_group_size)), tf.float32), trainable=False)\n",
    "        \n",
    "        super(KerasCustomMappingLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape): # since this is a static layer, no building is required\n",
    "        pass\n",
    "    \n",
    "    def set_extvar(self, extvar):\n",
    "        self.extvar = extvar;\n",
    "        \n",
    "        # Populate extvar with the rel variable (this will modify the input extvar)\n",
    "        begin_offset = extvar[\"begin\"];\n",
    "        self.extvar[\"relevant_tensors\"] = {\n",
    "            \"is_slider\"       : tf.convert_to_tensor(is_slider      [begin_offset : begin_offset + self.note_group_size], dtype=tf.bool),\n",
    "            \"slider_lengths\"  : tf.convert_to_tensor(slider_lengths [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_types\"    : tf.convert_to_tensor(slider_types   [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_cos_each\" : tf.convert_to_tensor(slider_cos_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"slider_sin_each\" : tf.convert_to_tensor(slider_sin_each[begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_distances\" :  tf.convert_to_tensor(note_distances [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32),\n",
    "            \"note_angles\" :     tf.convert_to_tensor(note_angles    [begin_offset : begin_offset + self.note_group_size], dtype=tf.float32)\n",
    "        };\n",
    "        \n",
    "        # Continue\n",
    "        self.extvar_begin.assign(extvar[\"begin\"])\n",
    "        self.extvar_spos.assign(extvar[\"start_pos\"])\n",
    "        self.extvar_lmul.assign([extvar[\"length_multiplier\"]])\n",
    "        self.extvar_nfse.assign(extvar[\"next_from_slider_end\"])\n",
    "        self.extvar_rel.assign(tf.convert_to_tensor([\n",
    "            is_slider      [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_lengths [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_types   [begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_cos_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            slider_sin_each[begin_offset : begin_offset + self.note_group_size],\n",
    "            note_distances [begin_offset : begin_offset + self.note_group_size],\n",
    "            note_angles    [begin_offset : begin_offset + self.note_group_size]\n",
    "        ], dtype=tf.float32))\n",
    "\n",
    "    # Call method will sometimes get used in graph mode,\n",
    "    # training will get turned into a tensor\n",
    "#     @tf.function\n",
    "    def call(self, inputs, training=None):\n",
    "        mapvars = inputs;\n",
    "        start_pos = self.extvar_spos\n",
    "        rel = self.extvar_rel\n",
    "        extvar = {\n",
    "            \"begin\" : self.extvar_begin,\n",
    "            # \"start_pos\" : self.extvar_start_pos,\n",
    "            \"start_pos\" : tf.cast(start_pos, tf.float32),\n",
    "            \"length_multiplier\" : self.extvar_lmul,\n",
    "            \"next_from_slider_end\" : self.extvar_nfse,\n",
    "            # \"relevant_tensors\" : self.extvar_rel\n",
    "            \"relevant_tensors\" : {\n",
    "                \"is_slider\"       : tf.cast(rel[0], tf.bool),\n",
    "                \"slider_lengths\"  : tf.cast(rel[1], tf.float32),\n",
    "                \"slider_types\"    : tf.cast(rel[2], tf.float32),\n",
    "                \"slider_cos_each\" : tf.cast(rel[3], tf.float32),\n",
    "                \"slider_sin_each\" : tf.cast(rel[4], tf.float32),\n",
    "                \"note_distances\"  : tf.cast(rel[5], tf.float32),\n",
    "                \"note_angles\"     : tf.cast(rel[6], tf.float32)\n",
    "            }\n",
    "        }\n",
    "        result = construct_map_with_sliders(mapvars, extvar=extvar);\n",
    "        return result;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "RwWPaJryD2aN"
   },
   "source": [
    "Now we can train the model!\n",
    "\n",
    "This will take some time. It splits map into groups of 10 (10 by default), and trains GAN to play with the flow of maps in the dataset. It will train for floor(note_count / 10) groups, for example, if you have 820 then it needs to train 82 groups.\n",
    "\n",
    "Some notes are converted to sliders, so it has less than the predicted count in #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "XdfkR223D9dW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of groups: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 12:45:36.950327  5384 training.py:1952] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0, Epoch 1: G loss: 0.4520745396614075 vs. C loss: 0.1879090236292945\n",
      "Group 0, Epoch 2: G loss: 0.4310892071042742 vs. C loss: 0.10930400010612275\n",
      "Group 0, Epoch 3: G loss: 0.37248848293508807 vs. C loss: 0.21794981178310183\n",
      "Group 0, Epoch 4: G loss: 0.030238568782806396 vs. C loss: 0.2052962490253978\n",
      "Group 0, Epoch 5: G loss: 0.008487080010984626 vs. C loss: 0.20642117493682435\n",
      "Group 0, Epoch 6: G loss: 0.0062955374829471115 vs. C loss: 0.20582916835943857\n",
      "Group 1, Epoch 1: G loss: 0.4503402752535684 vs. C loss: 0.24436351325776842\n",
      "Group 1, Epoch 2: G loss: 0.4182482004165649 vs. C loss: 0.14825020316574308\n",
      "Group 1, Epoch 3: G loss: 0.2511786584343229 vs. C loss: 0.1542669501569536\n",
      "Group 1, Epoch 4: G loss: 0.3739763877221515 vs. C loss: 0.23942395051320395\n",
      "Group 1, Epoch 5: G loss: 0.052548174613288465 vs. C loss: 0.20446584787633684\n",
      "Group 1, Epoch 6: G loss: 0.0210205544850656 vs. C loss: 0.20543441673119864\n",
      "Group 2, Epoch 1: G loss: 0.43978365404265274 vs. C loss: 0.2521318296591441\n",
      "Group 2, Epoch 2: G loss: 0.2923184663057327 vs. C loss: 0.1770518405569924\n",
      "Group 2, Epoch 3: G loss: 0.24453179112502507 vs. C loss: 0.14417316930161583\n",
      "Group 2, Epoch 4: G loss: 0.5165249219962529 vs. C loss: 0.11134661402967243\n",
      "Group 2, Epoch 5: G loss: 0.26552968855415077 vs. C loss: 0.2180190823144383\n",
      "Group 2, Epoch 6: G loss: 0.036932847010237836 vs. C loss: 0.20562235348754457\n",
      "Group 3, Epoch 1: G loss: 0.4441869352545057 vs. C loss: 0.2519679996702406\n",
      "Group 3, Epoch 2: G loss: 0.2844076603651047 vs. C loss: 0.18421710696485305\n",
      "Group 3, Epoch 3: G loss: 0.1686612510255405 vs. C loss: 0.16474689129326078\n",
      "Group 3, Epoch 4: G loss: 0.32469478419848846 vs. C loss: 0.16992017957899305\n",
      "Group 3, Epoch 5: G loss: 0.3089837410620281 vs. C loss: 0.21197990576426187\n",
      "Group 3, Epoch 6: G loss: 0.06265294839228902 vs. C loss: 0.2003000792529848\n",
      "Group 4, Epoch 1: G loss: 0.4166563119207109 vs. C loss: 0.26153063442971974\n",
      "Group 4, Epoch 2: G loss: 0.37222894941057477 vs. C loss: 0.18954410817888045\n",
      "Group 4, Epoch 3: G loss: 0.17083641205515177 vs. C loss: 0.16443260055449274\n",
      "Group 4, Epoch 4: G loss: 0.2804768545286996 vs. C loss: 0.1288884828488032\n",
      "Group 4, Epoch 5: G loss: 0.40325402830328255 vs. C loss: 0.1741318909658326\n",
      "Group 4, Epoch 6: G loss: 0.28270061825002946 vs. C loss: 0.13890746401415932\n",
      "Group 4, Epoch 7: G loss: 0.5415998297078269 vs. C loss: 0.07180954929855135\n",
      "Group 4, Epoch 8: G loss: 0.7222465651375906 vs. C loss: 0.04661039428578483\n",
      "Group 4, Epoch 9: G loss: 0.7914043733051844 vs. C loss: 0.008612941704793936\n",
      "Group 4, Epoch 10: G loss: 0.9290535296712603 vs. C loss: 0.0057112609243227365\n",
      "Group 4, Epoch 11: G loss: 0.7371359731469836 vs. C loss: 0.04276916364000904\n",
      "Group 4, Epoch 12: G loss: 0.3693639268832548 vs. C loss: 0.2242667939927843\n",
      "Group 4, Epoch 13: G loss: 0.05141796650631087 vs. C loss: 0.21934900598393547\n",
      "Group 4, Epoch 14: G loss: 0.042742211903844564 vs. C loss: 0.20830519497394562\n",
      "Group 4, Epoch 15: G loss: 0.03123224526643753 vs. C loss: 0.20831232021252313\n",
      "Group 4, Epoch 16: G loss: 0.020588149183562825 vs. C loss: 0.20831459760665894\n",
      "Group 4, Epoch 17: G loss: 0.019310593019638743 vs. C loss: 0.20831543621089724\n",
      "Group 4, Epoch 18: G loss: 0.01873830051294395 vs. C loss: 0.2083156555891037\n",
      "Group 4, Epoch 19: G loss: 0.017351180261799267 vs. C loss: 0.20831575824154747\n",
      "Group 4, Epoch 20: G loss: 0.017305335083178112 vs. C loss: 0.20831567835476664\n",
      "Group 4, Epoch 21: G loss: 0.01730491444468498 vs. C loss: 0.2083154817422231\n",
      "Group 4, Epoch 22: G loss: 0.01730484254658222 vs. C loss: 0.20831524001227483\n",
      "Group 4, Epoch 23: G loss: 0.01730486016188349 vs. C loss: 0.2083150504363908\n",
      "Group 4, Epoch 24: G loss: 0.017305178993514605 vs. C loss: 0.2083146928085221\n",
      "Group 4, Epoch 25: G loss: 0.01730557967509542 vs. C loss: 0.20831418327159354\n",
      "Group 5, Epoch 1: G loss: 0.45461295672825397 vs. C loss: 0.25509398182233173\n",
      "Group 5, Epoch 2: G loss: 0.2551499149629048 vs. C loss: 0.20563810401492646\n",
      "Group 5, Epoch 3: G loss: 0.07262877609048571 vs. C loss: 0.1831589953766929\n",
      "Group 5, Epoch 4: G loss: 0.16667028537818365 vs. C loss: 0.12239754779471292\n",
      "Group 5, Epoch 5: G loss: 0.6806348204612733 vs. C loss: 0.09546366665098403\n",
      "Group 5, Epoch 6: G loss: 0.47233868071011137 vs. C loss: 0.07657658805449803\n",
      "Group 6, Epoch 1: G loss: 0.48761356983866005 vs. C loss: 0.22158905367056533\n",
      "Group 6, Epoch 2: G loss: 0.45227158750806534 vs. C loss: 0.10717285258902444\n",
      "Group 6, Epoch 3: G loss: 0.44736204147338865 vs. C loss: 0.15896852066119513\n",
      "Group 6, Epoch 4: G loss: 0.6417941161564418 vs. C loss: 0.059852633211347796\n",
      "Group 6, Epoch 5: G loss: 0.9412204367773873 vs. C loss: 0.01751860407077604\n",
      "Group 6, Epoch 6: G loss: 0.8658287661416191 vs. C loss: 0.0019523813906643123\n",
      "Group 6, Epoch 7: G loss: 0.9925153204372952 vs. C loss: 0.0012203657922024529\n",
      "Group 7, Epoch 1: G loss: 0.4927608958312443 vs. C loss: 0.23117168413268196\n",
      "Group 7, Epoch 2: G loss: 0.3574495213372367 vs. C loss: 0.15803391858935356\n",
      "Group 7, Epoch 3: G loss: 0.37913775018283297 vs. C loss: 0.12560358477963343\n",
      "Group 7, Epoch 4: G loss: 0.769366626228605 vs. C loss: 0.11244938212136428\n",
      "Group 7, Epoch 5: G loss: 0.17429347283073834 vs. C loss: 0.2058509017030398\n",
      "Group 7, Epoch 6: G loss: 0.03341424294880459 vs. C loss: 0.2077957722875807\n",
      "Group 8, Epoch 1: G loss: 0.4969583034515381 vs. C loss: 0.24514607091744742\n",
      "Group 8, Epoch 2: G loss: 0.3537844521658761 vs. C loss: 0.13048943960004383\n",
      "Group 8, Epoch 3: G loss: 0.46026237692151745 vs. C loss: 0.1253973013824887\n",
      "Group 8, Epoch 4: G loss: 0.4035093137196132 vs. C loss: 0.2108567754427592\n",
      "Group 8, Epoch 5: G loss: 0.051178234070539466 vs. C loss: 0.20585642837815818\n",
      "Group 8, Epoch 6: G loss: 0.04743668362498284 vs. C loss: 0.2058640155527327\n",
      "Group 8, Epoch 7: G loss: 0.0648807707641806 vs. C loss: 0.20202494909365973\n",
      "Group 8, Epoch 8: G loss: 0.07258535261665074 vs. C loss: 0.17212110923396218\n",
      "Group 9, Epoch 1: G loss: 0.4750162056514195 vs. C loss: 0.25424345831076306\n",
      "Group 9, Epoch 2: G loss: 0.3740583547524043 vs. C loss: 0.1876336129175292\n",
      "Group 9, Epoch 3: G loss: 0.2260158598423004 vs. C loss: 0.1378466652499305\n",
      "Group 9, Epoch 4: G loss: 0.5326175263949803 vs. C loss: 0.052138915906349816\n",
      "Group 9, Epoch 5: G loss: 0.8147664717265538 vs. C loss: 0.020482972471250426\n",
      "Group 9, Epoch 6: G loss: 0.7798678295952932 vs. C loss: 0.009011315595772531\n",
      "Group 10, Epoch 1: G loss: 0.44430713398115973 vs. C loss: 0.2466655969619751\n",
      "Group 10, Epoch 2: G loss: 0.25748262192521776 vs. C loss: 0.19260831342803109\n",
      "Group 10, Epoch 3: G loss: 0.07708332687616348 vs. C loss: 0.19097783499293855\n",
      "Group 10, Epoch 4: G loss: 0.13373748915536063 vs. C loss: 0.12972979082001582\n",
      "Group 10, Epoch 5: G loss: 0.6013279727527073 vs. C loss: 0.06477550541361173\n",
      "Group 10, Epoch 6: G loss: 0.726338699885777 vs. C loss: 0.09387906392415364\n",
      "Group 10, Epoch 7: G loss: 0.6598873819623675 vs. C loss: 0.1269771961702241\n",
      "Group 11, Epoch 1: G loss: 0.42339119485446386 vs. C loss: 0.25434809923172\n",
      "Group 11, Epoch 2: G loss: 0.3290992208889552 vs. C loss: 0.17925796740584907\n",
      "Group 11, Epoch 3: G loss: 0.3203122207096644 vs. C loss: 0.13141801870531505\n",
      "Group 11, Epoch 4: G loss: 0.6066671226705823 vs. C loss: 0.09356245232952966\n",
      "Group 11, Epoch 5: G loss: 0.44718207631792334 vs. C loss: 0.1910449647241169\n",
      "Group 11, Epoch 6: G loss: 0.2113024309277535 vs. C loss: 0.17320608264870116\n",
      "Group 11, Epoch 7: G loss: 0.7448960202080863 vs. C loss: 0.051111615366405905\n",
      "Group 11, Epoch 8: G loss: 0.4937397829123905 vs. C loss: 0.17900069057941437\n",
      "Group 12, Epoch 1: G loss: 0.46597524200166973 vs. C loss: 0.2584717902872298\n",
      "Group 12, Epoch 2: G loss: 0.44632208262171064 vs. C loss: 0.14444714784622192\n",
      "Group 12, Epoch 3: G loss: 0.34020901066916337 vs. C loss: 0.1782819628715515\n",
      "Group 12, Epoch 4: G loss: 0.2414394308413778 vs. C loss: 0.21495889292822942\n",
      "Group 12, Epoch 5: G loss: 0.045769475400447845 vs. C loss: 0.20383498900466493\n",
      "Group 12, Epoch 6: G loss: 0.03822946112070765 vs. C loss: 0.1987625410159429\n",
      "Group 13, Epoch 1: G loss: 0.5256243765354157 vs. C loss: 0.24612445632616678\n",
      "Group 13, Epoch 2: G loss: 0.31009475673948017 vs. C loss: 0.16428602072927687\n",
      "Group 13, Epoch 3: G loss: 0.30264062881469733 vs. C loss: 0.1031419609983762\n",
      "Group 13, Epoch 4: G loss: 0.5334455260208675 vs. C loss: 0.06733879529767565\n",
      "Group 13, Epoch 5: G loss: 0.6092901566198894 vs. C loss: 0.09645664733317162\n",
      "Group 13, Epoch 6: G loss: 0.554869848064014 vs. C loss: 0.11977192221416366\n",
      "Group 14, Epoch 1: G loss: 0.49710057462964746 vs. C loss: 0.23479304545455507\n",
      "Group 14, Epoch 2: G loss: 0.3400069692305156 vs. C loss: 0.15212314989831713\n",
      "Group 14, Epoch 3: G loss: 0.4017962736742837 vs. C loss: 0.1288457852270868\n",
      "Group 14, Epoch 4: G loss: 0.4229701727628708 vs. C loss: 0.19249196764495638\n",
      "Group 14, Epoch 5: G loss: 0.1300470790692738 vs. C loss: 0.154431422551473\n",
      "Group 14, Epoch 6: G loss: 0.6486146245683944 vs. C loss: 0.06275780209236674\n",
      "Group 15, Epoch 1: G loss: 0.5021064111164638 vs. C loss: 0.2620196474923028\n",
      "Group 15, Epoch 2: G loss: 0.4381814752306257 vs. C loss: 0.14519362317191228\n",
      "Group 15, Epoch 3: G loss: 0.3112122156790325 vs. C loss: 0.1473583628733953\n",
      "Group 15, Epoch 4: G loss: 0.3399613480482783 vs. C loss: 0.21167753802405465\n",
      "Group 15, Epoch 5: G loss: 0.052289175455059324 vs. C loss: 0.20840944432550004\n",
      "Group 15, Epoch 6: G loss: 0.0424673216683524 vs. C loss: 0.20421711438231996\n",
      "Group 16, Epoch 1: G loss: 0.4444098498140062 vs. C loss: 0.25098751485347753\n",
      "Group 16, Epoch 2: G loss: 0.30084575499807087 vs. C loss: 0.18180509242746568\n",
      "Group 16, Epoch 3: G loss: 0.15333062367779868 vs. C loss: 0.17816510796546936\n",
      "Group 16, Epoch 4: G loss: 0.13740042320319584 vs. C loss: 0.16506355504194895\n",
      "Group 16, Epoch 5: G loss: 0.27928658383233207 vs. C loss: 0.16306154843833712\n",
      "Group 16, Epoch 6: G loss: 0.3597313685076577 vs. C loss: 0.1394311934709549\n",
      "Group 17, Epoch 1: G loss: 0.44460856063025334 vs. C loss: 0.26002247134844464\n",
      "Group 17, Epoch 2: G loss: 0.3181823568684714 vs. C loss: 0.17745870434575609\n",
      "Group 17, Epoch 3: G loss: 0.28969478096280776 vs. C loss: 0.1266381119688352\n",
      "Group 17, Epoch 4: G loss: 0.462819584778377 vs. C loss: 0.08855227670735782\n",
      "Group 17, Epoch 5: G loss: 0.6011819473334722 vs. C loss: 0.05311367909113566\n",
      "Group 17, Epoch 6: G loss: 0.6168332182935305 vs. C loss: 0.21873909566137528\n",
      "Group 17, Epoch 7: G loss: 0.02723550737968513 vs. C loss: 0.208024897509151\n",
      "Group 18, Epoch 1: G loss: 0.44473022137369433 vs. C loss: 0.2460423459609349\n",
      "Group 18, Epoch 2: G loss: 0.3481039511305945 vs. C loss: 0.15321190738015705\n",
      "Group 18, Epoch 3: G loss: 0.41058475673198697 vs. C loss: 0.10574679200847943\n",
      "Group 18, Epoch 4: G loss: 0.8435995646885462 vs. C loss: 0.03097455131096972\n",
      "Group 18, Epoch 5: G loss: 0.5923238677637918 vs. C loss: 0.028407104830774993\n",
      "Group 18, Epoch 6: G loss: 0.9904100605419704 vs. C loss: 0.0288991660086645\n",
      "Group 19, Epoch 1: G loss: 0.5134855117116656 vs. C loss: 0.2456817213031981\n",
      "Group 19, Epoch 2: G loss: 0.279327906029565 vs. C loss: 0.15744165910614863\n",
      "Group 19, Epoch 3: G loss: 0.3366956949234009 vs. C loss: 0.11820130050182343\n",
      "Group 19, Epoch 4: G loss: 0.6108391165733337 vs. C loss: 0.08103608008888033\n",
      "Group 19, Epoch 5: G loss: 0.5774881873811994 vs. C loss: 0.055710244095987745\n",
      "Group 19, Epoch 6: G loss: 0.8432562862123761 vs. C loss: 0.04768989504211479\n",
      "Group 20, Epoch 1: G loss: 0.5040102405207497 vs. C loss: 0.24866568048795065\n",
      "Group 20, Epoch 2: G loss: 0.3024572840758732 vs. C loss: 0.17019370529386732\n",
      "Group 20, Epoch 3: G loss: 0.2976974368095398 vs. C loss: 0.08281351377566655\n",
      "Group 20, Epoch 4: G loss: 0.7935940708432879 vs. C loss: 0.03141253886537419\n",
      "Group 20, Epoch 5: G loss: 0.8471062506948198 vs. C loss: 0.013125555848495828\n",
      "Group 20, Epoch 6: G loss: 0.8814424037933348 vs. C loss: 0.011572822385157147\n",
      "Group 21, Epoch 1: G loss: 0.43494909831455775 vs. C loss: 0.24803754190603897\n",
      "Group 21, Epoch 2: G loss: 0.37701589550290787 vs. C loss: 0.1478633772995737\n",
      "Group 21, Epoch 3: G loss: 0.3003793218306133 vs. C loss: 0.18028424349096087\n",
      "Group 21, Epoch 4: G loss: 0.37952818870544436 vs. C loss: 0.10735005140304565\n",
      "Group 21, Epoch 5: G loss: 0.6691721320152283 vs. C loss: 0.04048900306224823\n",
      "Group 21, Epoch 6: G loss: 0.8334136268922262 vs. C loss: 0.11333535859982173\n",
      "Group 21, Epoch 7: G loss: 0.5706362364547594 vs. C loss: 0.262707069516182\n",
      "Group 21, Epoch 8: G loss: 0.06086294023053986 vs. C loss: 0.2059709247615602\n",
      "Group 21, Epoch 9: G loss: 0.05214352032967976 vs. C loss: 0.20486408803198075\n",
      "Group 21, Epoch 10: G loss: 0.038268561235495976 vs. C loss: 0.2018448660771052\n",
      "Group 22, Epoch 1: G loss: 0.39975998912538796 vs. C loss: 0.26565876271989614\n",
      "Group 22, Epoch 2: G loss: 0.29788149297237393 vs. C loss: 0.20542413906918633\n",
      "Group 22, Epoch 3: G loss: 0.08166081032582694 vs. C loss: 0.1980984111626943\n",
      "Group 22, Epoch 4: G loss: 0.06875682015504156 vs. C loss: 0.1836679197020001\n",
      "Group 22, Epoch 5: G loss: 0.16985513099602292 vs. C loss: 0.1562648249997033\n",
      "Group 22, Epoch 6: G loss: 0.3179073018687112 vs. C loss: 0.1693607916434606\n",
      "Group 23, Epoch 1: G loss: 0.47500613076346265 vs. C loss: 0.24987636672125924\n",
      "Group 23, Epoch 2: G loss: 0.3254624783992767 vs. C loss: 0.1676754413379563\n",
      "Group 23, Epoch 3: G loss: 0.35312994377953666 vs. C loss: 0.14767753415637547\n",
      "Group 23, Epoch 4: G loss: 0.47323383944375175 vs. C loss: 0.10827757583724128\n",
      "Group 23, Epoch 5: G loss: 0.4253104431288583 vs. C loss: 0.17994054986370936\n",
      "Group 23, Epoch 6: G loss: 0.3779187338692802 vs. C loss: 0.1498215703500642\n",
      "Group 23, Epoch 7: G loss: 0.5660911830408233 vs. C loss: 0.22761026935444936\n",
      "Group 24, Epoch 1: G loss: 0.5085837483406067 vs. C loss: 0.27163871791627675\n",
      "Group 24, Epoch 2: G loss: 0.3542512306145259 vs. C loss: 0.1671697927845849\n",
      "Group 24, Epoch 3: G loss: 0.3258987712008613 vs. C loss: 0.11945820848147075\n",
      "Group 24, Epoch 4: G loss: 0.5784990515027727 vs. C loss: 0.08344388711783622\n",
      "Group 24, Epoch 5: G loss: 0.7801265818732126 vs. C loss: 0.015202313325264389\n",
      "Group 24, Epoch 6: G loss: 0.8395114387784686 vs. C loss: 0.029087480157613754\n",
      "Group 24, Epoch 7: G loss: 0.9714274338313512 vs. C loss: 0.02316429385811918\n",
      "Group 25, Epoch 1: G loss: 0.4602472049849374 vs. C loss: 0.25191959076457554\n",
      "Group 25, Epoch 2: G loss: 0.3187738324914659 vs. C loss: 0.17582114951478112\n",
      "Group 25, Epoch 3: G loss: 0.18777314765112743 vs. C loss: 0.13285494968295097\n",
      "Group 25, Epoch 4: G loss: 0.4675210731370108 vs. C loss: 0.11834749496645397\n",
      "Group 25, Epoch 5: G loss: 0.7096037677356175 vs. C loss: 0.11935770221882396\n",
      "Group 25, Epoch 6: G loss: 0.3623779382024493 vs. C loss: 0.07239362721641858\n",
      "Group 26, Epoch 1: G loss: 0.533076411485672 vs. C loss: 0.23306897613737318\n",
      "Group 26, Epoch 2: G loss: 0.47192663635526383 vs. C loss: 0.10773519261015786\n",
      "Group 26, Epoch 3: G loss: 0.2496407333229269 vs. C loss: 0.2186089621649848\n",
      "Group 26, Epoch 4: G loss: 0.03993097761912006 vs. C loss: 0.2001935988664627\n",
      "Group 26, Epoch 5: G loss: 0.04184021172778947 vs. C loss: 0.1935395449399948\n",
      "Group 26, Epoch 6: G loss: 0.11336196426834379 vs. C loss: 0.15097617275185055\n",
      "Group 27, Epoch 1: G loss: 0.42612723197255814 vs. C loss: 0.2649678554799822\n",
      "Group 27, Epoch 2: G loss: 0.42290899923869546 vs. C loss: 0.13578548861874476\n",
      "Group 27, Epoch 3: G loss: 0.3115061664155551 vs. C loss: 0.20339692797925737\n",
      "Group 27, Epoch 4: G loss: 0.08864037947995321 vs. C loss: 0.19143591076135635\n",
      "Group 27, Epoch 5: G loss: 0.11548887065478733 vs. C loss: 0.17194802893532646\n",
      "Group 27, Epoch 6: G loss: 0.3726163114820208 vs. C loss: 0.11801623635821873\n",
      "Group 28, Epoch 1: G loss: 0.42967448234558103 vs. C loss: 0.2540042400360107\n",
      "Group 28, Epoch 2: G loss: 0.2821849720818656 vs. C loss: 0.18108540607823267\n",
      "Group 28, Epoch 3: G loss: 0.1746656794633184 vs. C loss: 0.17929084599018097\n",
      "Group 28, Epoch 4: G loss: 0.28720878830977853 vs. C loss: 0.15687879588868883\n",
      "Group 28, Epoch 5: G loss: 0.30339249713080274 vs. C loss: 0.17886245747407278\n",
      "Group 28, Epoch 6: G loss: 0.20770236296313146 vs. C loss: 0.18948788775338066\n",
      "Group 29, Epoch 1: G loss: 0.4941109137875693 vs. C loss: 0.2616185413466559\n",
      "Group 29, Epoch 2: G loss: 0.3494418842451913 vs. C loss: 0.17416073381900787\n",
      "Group 29, Epoch 3: G loss: 0.24585218897887642 vs. C loss: 0.16644883900880814\n",
      "Group 29, Epoch 4: G loss: 0.2484870425292424 vs. C loss: 0.18569831218984392\n",
      "Group 29, Epoch 5: G loss: 0.23521688495363507 vs. C loss: 0.1477323994040489\n",
      "Group 29, Epoch 6: G loss: 0.4628819669995989 vs. C loss: 0.19430177741580537\n",
      "Group 29, Epoch 7: G loss: 0.27546131099973403 vs. C loss: 0.18577626264757582\n",
      "Group 30, Epoch 1: G loss: 0.5203184306621551 vs. C loss: 0.24147244294484457\n",
      "Group 30, Epoch 2: G loss: 0.36566430245126996 vs. C loss: 0.18296016421582964\n",
      "Group 30, Epoch 3: G loss: 0.19170297511986326 vs. C loss: 0.18643154038323298\n",
      "Group 30, Epoch 4: G loss: 0.18262446416275843 vs. C loss: 0.2088175556725926\n",
      "Group 30, Epoch 5: G loss: 0.06413618549704551 vs. C loss: 0.18645141025384268\n",
      "Group 30, Epoch 6: G loss: 0.13506741097995212 vs. C loss: 0.15876685082912445\n",
      "Group 31, Epoch 1: G loss: 0.4783504486083984 vs. C loss: 0.25303724408149714\n",
      "Group 31, Epoch 2: G loss: 0.38748581494603834 vs. C loss: 0.13515947262446085\n",
      "Group 31, Epoch 3: G loss: 0.39019305578299923 vs. C loss: 0.1324190605017874\n",
      "Group 31, Epoch 4: G loss: 0.7866337282317026 vs. C loss: 0.027956289963589776\n",
      "Group 31, Epoch 5: G loss: 0.8644752246992928 vs. C loss: 0.018642155970964167\n",
      "Group 31, Epoch 6: G loss: 0.8280427281345639 vs. C loss: 0.092127226293087\n",
      "Group 32, Epoch 1: G loss: 0.4794623008796147 vs. C loss: 0.26052634583579165\n",
      "Group 32, Epoch 2: G loss: 0.37494499683380134 vs. C loss: 0.17176283233695558\n",
      "Group 32, Epoch 3: G loss: 0.22961606425898415 vs. C loss: 0.12618596934609944\n",
      "Group 32, Epoch 4: G loss: 0.6127356112003327 vs. C loss: 0.0840475286046664\n",
      "Group 32, Epoch 5: G loss: 0.613292613199779 vs. C loss: 0.19909426735507116\n",
      "Group 32, Epoch 6: G loss: 0.217597045642989 vs. C loss: 0.14133183823691473\n",
      "Group 32, Epoch 7: G loss: 0.9153792228017535 vs. C loss: 0.036503300691644355\n",
      "Group 32, Epoch 8: G loss: 0.6889106137411936 vs. C loss: 0.028097557731800605\n",
      "Group 32, Epoch 9: G loss: 0.8749723178999765 vs. C loss: 0.021272185775968764\n",
      "Group 33, Epoch 1: G loss: 0.44185063924108237 vs. C loss: 0.2687489655282762\n",
      "Group 33, Epoch 2: G loss: 0.3174423213515963 vs. C loss: 0.19460997151003945\n",
      "Group 33, Epoch 3: G loss: 0.1409180951969964 vs. C loss: 0.17860373792548975\n",
      "Group 33, Epoch 4: G loss: 0.21273976053510393 vs. C loss: 0.1306742214494281\n",
      "Group 33, Epoch 5: G loss: 0.5588370595659529 vs. C loss: 0.10769048747089173\n",
      "Group 33, Epoch 6: G loss: 0.3112654490130288 vs. C loss: 0.19582845601770613\n",
      "Group 34, Epoch 1: G loss: 0.4212032539503915 vs. C loss: 0.26689041488700443\n",
      "Group 34, Epoch 2: G loss: 0.3518248004572732 vs. C loss: 0.1456488896575239\n",
      "Group 34, Epoch 3: G loss: 0.29320388010569987 vs. C loss: 0.1272053552998437\n",
      "Group 34, Epoch 4: G loss: 0.42179428083556036 vs. C loss: 0.13699960543049708\n",
      "Group 34, Epoch 5: G loss: 0.47252231410571505 vs. C loss: 0.11808814687861335\n",
      "Group 34, Epoch 6: G loss: 0.7294874344553267 vs. C loss: 0.0340353741000096\n",
      "Group 35, Epoch 1: G loss: 0.47878125224794665 vs. C loss: 0.24897148377365533\n",
      "Group 35, Epoch 2: G loss: 0.3221612117120198 vs. C loss: 0.18071176608403525\n",
      "Group 35, Epoch 3: G loss: 0.1533375118459974 vs. C loss: 0.17104511294100022\n",
      "Group 35, Epoch 4: G loss: 0.23692656925746372 vs. C loss: 0.15430339177449545\n",
      "Group 35, Epoch 5: G loss: 0.5397370576858521 vs. C loss: 0.08726258513828118\n",
      "Group 35, Epoch 6: G loss: 0.5279294690915517 vs. C loss: 0.09507733831803004\n",
      "Group 36, Epoch 1: G loss: 0.4729754311697824 vs. C loss: 0.23930074108971489\n",
      "Group 36, Epoch 2: G loss: 0.39249510765075685 vs. C loss: 0.1234300807118416\n",
      "Group 36, Epoch 3: G loss: 0.4885980657168797 vs. C loss: 0.100334828098615\n",
      "Group 36, Epoch 4: G loss: 0.46551312889371593 vs. C loss: 0.09756226630674468\n",
      "Group 36, Epoch 5: G loss: 0.7832379528454372 vs. C loss: 0.0779720897682839\n",
      "Group 36, Epoch 6: G loss: 0.38916861297828814 vs. C loss: 0.196719605061743\n",
      "Group 36, Epoch 7: G loss: 0.058610044526202346 vs. C loss: 0.19074387351671854\n",
      "Group 37, Epoch 1: G loss: 0.47869882157870697 vs. C loss: 0.261862619055642\n",
      "Group 37, Epoch 2: G loss: 0.3622177217687879 vs. C loss: 0.1837840990887748\n",
      "Group 37, Epoch 3: G loss: 0.3159284370286124 vs. C loss: 0.10829460952017043\n",
      "Group 37, Epoch 4: G loss: 0.559319383757455 vs. C loss: 0.10095693253808552\n",
      "Group 37, Epoch 5: G loss: 0.7136092373303003 vs. C loss: 0.05602423060271475\n",
      "Group 37, Epoch 6: G loss: 0.6901523521968296 vs. C loss: 0.013097593198633857\n",
      "Group 38, Epoch 1: G loss: 0.4902640180928367 vs. C loss: 0.24716874294810823\n",
      "Group 38, Epoch 2: G loss: 0.3772628128528595 vs. C loss: 0.17236180437935722\n",
      "Group 38, Epoch 3: G loss: 0.2413601960454668 vs. C loss: 0.12781330363618004\n",
      "Group 38, Epoch 4: G loss: 0.6456845249448504 vs. C loss: 0.06325420695874426\n",
      "Group 38, Epoch 5: G loss: 0.6878735678536552 vs. C loss: 0.02486331719491217\n",
      "Group 38, Epoch 6: G loss: 0.959010216167995 vs. C loss: 0.011007570635734333\n",
      "Group 39, Epoch 1: G loss: 0.5034295763288226 vs. C loss: 0.233925504816903\n",
      "Group 39, Epoch 2: G loss: 0.2850896647998265 vs. C loss: 0.14230734689368144\n",
      "Group 39, Epoch 3: G loss: 0.424315402337483 vs. C loss: 0.08082762236396472\n",
      "Group 39, Epoch 4: G loss: 0.7317121999604362 vs. C loss: 0.041487589685453304\n",
      "Group 39, Epoch 5: G loss: 0.8442429167883737 vs. C loss: 0.012200371051828066\n",
      "Group 39, Epoch 6: G loss: 0.8706805791173663 vs. C loss: 0.007183108090733488\n",
      "Group 39, Epoch 7: G loss: 0.7361608807529721 vs. C loss: 0.19657480385568407\n",
      "Group 39, Epoch 8: G loss: 0.12054874833141056 vs. C loss: 0.2246927685207791\n",
      "Group 39, Epoch 9: G loss: 0.03806695767811367 vs. C loss: 0.20978725949923197\n",
      "Group 39, Epoch 10: G loss: 0.032787723307098664 vs. C loss: 0.20718261102835336\n",
      "Group 39, Epoch 11: G loss: 0.03095262902123588 vs. C loss: 0.2067837251557244\n",
      "Group 39, Epoch 12: G loss: 0.031484710531575345 vs. C loss: 0.20585644079579246\n",
      "Group 39, Epoch 13: G loss: 0.03580807905111994 vs. C loss: 0.20275775757100847\n",
      "Group 39, Epoch 14: G loss: 0.05171154599104609 vs. C loss: 0.19367671012878418\n",
      "Group 39, Epoch 15: G loss: 0.1279501768095153 vs. C loss: 0.1572602747215165\n",
      "Group 39, Epoch 16: G loss: 0.44953118051801405 vs. C loss: 0.2792221440209283\n",
      "Group 39, Epoch 17: G loss: 0.07008347873176847 vs. C loss: 0.19331308868196273\n",
      "Group 39, Epoch 18: G loss: 0.22804115968091146 vs. C loss: 0.11681675869557594\n",
      "Group 39, Epoch 19: G loss: 0.6735783457756043 vs. C loss: 0.041862970011101834\n",
      "Group 39, Epoch 20: G loss: 0.5749338729040964 vs. C loss: 0.033134487819754414\n",
      "Group 39, Epoch 21: G loss: 0.9916910682405744 vs. C loss: 0.00757500170988755\n",
      "Group 39, Epoch 22: G loss: 0.9439560226031711 vs. C loss: 0.05081670257237015\n",
      "Group 39, Epoch 23: G loss: 0.9516764794077192 vs. C loss: 0.013867218668262163\n",
      "Group 39, Epoch 24: G loss: 0.5683981545801673 vs. C loss: 0.20991188950008818\n",
      "Group 40, Epoch 1: G loss: 0.49273275392396115 vs. C loss: 0.2537324047750897\n",
      "Group 40, Epoch 2: G loss: 0.345054304599762 vs. C loss: 0.17996318555540505\n",
      "Group 40, Epoch 3: G loss: 0.17071243694850377 vs. C loss: 0.1583415261573262\n",
      "Group 40, Epoch 4: G loss: 0.4109154079641614 vs. C loss: 0.09361409892638524\n",
      "Group 40, Epoch 5: G loss: 0.6424194906439099 vs. C loss: 0.08820046815607281\n",
      "Group 40, Epoch 6: G loss: 0.7000223909105572 vs. C loss: 0.13536440663867524\n",
      "Group 40, Epoch 7: G loss: 0.885770787511553 vs. C loss: 0.033775485224194\n",
      "Group 40, Epoch 8: G loss: 0.5107562456812177 vs. C loss: 0.05818916194968754\n",
      "Group 40, Epoch 9: G loss: 0.9845513837678092 vs. C loss: 0.014724916933725277\n",
      "Group 40, Epoch 10: G loss: 0.27962061730878696 vs. C loss: 0.20627149111694756\n",
      "Group 41, Epoch 1: G loss: 0.4681384912558964 vs. C loss: 0.25629886488119763\n",
      "Group 41, Epoch 2: G loss: 0.34657276783670704 vs. C loss: 0.17641038613186943\n",
      "Group 41, Epoch 3: G loss: 0.16150681631905692 vs. C loss: 0.14907014535533059\n",
      "Group 41, Epoch 4: G loss: 0.4146177087511335 vs. C loss: 0.08687115874555375\n",
      "Group 41, Epoch 5: G loss: 0.7666652968951634 vs. C loss: 0.04874732862744066\n",
      "Group 41, Epoch 6: G loss: 0.7547763620104109 vs. C loss: 0.04877353713123334\n",
      "Group 41, Epoch 7: G loss: 0.6720559750284467 vs. C loss: 0.040195185018496386\n",
      "Group 42, Epoch 1: G loss: 0.46612890533038553 vs. C loss: 0.25070663293202716\n",
      "Group 42, Epoch 2: G loss: 0.3967075424534934 vs. C loss: 0.1335643513335122\n",
      "Group 42, Epoch 3: G loss: 0.3836227927889143 vs. C loss: 0.09557010647323395\n",
      "Group 42, Epoch 4: G loss: 0.7710826754570006 vs. C loss: 0.034626932814717286\n",
      "Group 42, Epoch 5: G loss: 0.8013586776597158 vs. C loss: 0.02410711032441921\n",
      "Group 42, Epoch 6: G loss: 0.4624932012387685 vs. C loss: 0.2155302812655767\n",
      "Group 42, Epoch 7: G loss: 0.032873798161745064 vs. C loss: 0.19503858354356554\n",
      "Group 43, Epoch 1: G loss: 0.4607579742159162 vs. C loss: 0.2459932035870022\n",
      "Group 43, Epoch 2: G loss: 0.3544304200581142 vs. C loss: 0.1786792559756173\n",
      "Group 43, Epoch 3: G loss: 0.21760003694466182 vs. C loss: 0.15449903739823237\n",
      "Group 43, Epoch 4: G loss: 0.48714694551059173 vs. C loss: 0.09777260075012843\n",
      "Group 43, Epoch 5: G loss: 0.5838723727634975 vs. C loss: 0.06577920127246116\n",
      "Group 43, Epoch 6: G loss: 0.3220572820731572 vs. C loss: 0.09536604873008199\n",
      "Group 44, Epoch 1: G loss: 0.43468227812222077 vs. C loss: 0.2586686942312452\n",
      "Group 44, Epoch 2: G loss: 0.455731258222035 vs. C loss: 0.13606161458624733\n",
      "Group 44, Epoch 3: G loss: 0.6050568444388252 vs. C loss: 0.09152836600939433\n",
      "Group 44, Epoch 4: G loss: 0.364054295952831 vs. C loss: 0.19886841626268706\n",
      "Group 44, Epoch 5: G loss: 0.026606434051479614 vs. C loss: 0.2056041376458274\n",
      "Group 44, Epoch 6: G loss: 0.04260264411568642 vs. C loss: 0.19452730152342054\n",
      "Group 44, Epoch 7: G loss: 0.07086450085043908 vs. C loss: 0.1992342116104232\n",
      "Group 44, Epoch 8: G loss: 0.22878784068993158 vs. C loss: 0.100140367117193\n",
      "Group 44, Epoch 9: G loss: 0.6594866258757455 vs. C loss: 0.05531446304586199\n",
      "Group 44, Epoch 10: G loss: 0.6278193371636528 vs. C loss: 0.025323900523492034\n",
      "Group 44, Epoch 11: G loss: 0.9066007086208889 vs. C loss: 0.012491698997716108\n",
      "Group 44, Epoch 12: G loss: 0.8871956757136754 vs. C loss: 0.0038987501369168362\n",
      "Group 44, Epoch 13: G loss: 0.9079095704214913 vs. C loss: 0.004190737562667993\n",
      "Group 44, Epoch 14: G loss: 0.9815360069274902 vs. C loss: 0.01011432664462417\n",
      "Group 44, Epoch 15: G loss: 0.9858184099197388 vs. C loss: 0.0008064411781055644\n",
      "Group 44, Epoch 16: G loss: 0.43745330785002023 vs. C loss: 0.2060902333921856\n",
      "Group 44, Epoch 17: G loss: 0.023355594543474058 vs. C loss: 0.20808122472630605\n",
      "Group 45, Epoch 1: G loss: 0.4636063507625035 vs. C loss: 0.24754728708002302\n",
      "Group 45, Epoch 2: G loss: 0.36085878270012994 vs. C loss: 0.19089352918995753\n",
      "Group 45, Epoch 3: G loss: 0.20841300146920338 vs. C loss: 0.17068238473600808\n",
      "Group 45, Epoch 4: G loss: 0.34238444566726695 vs. C loss: 0.12949748502837288\n",
      "Group 45, Epoch 5: G loss: 0.4014925169093269 vs. C loss: 0.14969231602218416\n",
      "Group 45, Epoch 6: G loss: 0.31667029219014303 vs. C loss: 0.14803817205958897\n",
      "Group 46, Epoch 1: G loss: 0.43912161588668824 vs. C loss: 0.2613205131557253\n",
      "Group 46, Epoch 2: G loss: 0.29405557726110726 vs. C loss: 0.1778406964408027\n",
      "Group 46, Epoch 3: G loss: 0.23730668553284237 vs. C loss: 0.12574556718269983\n",
      "Group 46, Epoch 4: G loss: 0.6025906341416496 vs. C loss: 0.08624641348918279\n",
      "Group 46, Epoch 5: G loss: 0.5837499678134918 vs. C loss: 0.07121995960672697\n",
      "Group 46, Epoch 6: G loss: 0.6915080836841039 vs. C loss: 0.05732447043475177\n",
      "Group 46, Epoch 7: G loss: 0.8270935433251516 vs. C loss: 0.02801306381459451\n",
      "Group 47, Epoch 1: G loss: 0.4634887252535139 vs. C loss: 0.25127795835336053\n",
      "Group 47, Epoch 2: G loss: 0.2740246781281062 vs. C loss: 0.1724416439731916\n",
      "Group 47, Epoch 3: G loss: 0.2007917983191354 vs. C loss: 0.15926608443260193\n",
      "Group 47, Epoch 4: G loss: 0.24147983683007104 vs. C loss: 0.1690195037258996\n",
      "Group 47, Epoch 5: G loss: 0.31277268975973127 vs. C loss: 0.20806343522336745\n",
      "Group 47, Epoch 6: G loss: 0.09435112731797356 vs. C loss: 0.15827783942222595\n",
      "Group 48, Epoch 1: G loss: 0.492724598305566 vs. C loss: 0.2601531313525306\n",
      "Group 48, Epoch 2: G loss: 0.3786802138601031 vs. C loss: 0.19713507261541155\n",
      "Group 48, Epoch 3: G loss: 0.14956946671009064 vs. C loss: 0.1792994261615806\n",
      "Group 48, Epoch 4: G loss: 0.20745114002908976 vs. C loss: 0.1245517200893826\n",
      "Group 48, Epoch 5: G loss: 0.41563582292624884 vs. C loss: 0.14912360161542892\n",
      "Group 48, Epoch 6: G loss: 0.36148799040487833 vs. C loss: 0.18541670590639114\n",
      "Group 49, Epoch 1: G loss: 0.4793604144028255 vs. C loss: 0.24910955462190842\n",
      "Group 49, Epoch 2: G loss: 0.36971727439335417 vs. C loss: 0.1684631316198243\n",
      "Group 49, Epoch 3: G loss: 0.17518938226359232 vs. C loss: 0.19119132061799368\n",
      "Group 49, Epoch 4: G loss: 0.12180780214922769 vs. C loss: 0.15556386278735268\n",
      "Group 49, Epoch 5: G loss: 0.3704331585339138 vs. C loss: 0.11844716966152191\n",
      "Group 49, Epoch 6: G loss: 0.5980352197374617 vs. C loss: 0.05873206092251671\n",
      "Group 49, Epoch 7: G loss: 0.7431150470461164 vs. C loss: 0.1544191510313087\n",
      "Group 49, Epoch 8: G loss: 0.2890038428562028 vs. C loss: 0.23671226534578535\n",
      "Group 49, Epoch 9: G loss: 0.08206160877432142 vs. C loss: 0.20098875959714255\n",
      "Group 50, Epoch 1: G loss: 0.4950962858540671 vs. C loss: 0.2607120970884959\n",
      "Group 50, Epoch 2: G loss: 0.41028606551034114 vs. C loss: 0.1418210251463784\n",
      "Group 50, Epoch 3: G loss: 0.29043070546218325 vs. C loss: 0.1942960032158428\n",
      "Group 50, Epoch 4: G loss: 0.127530457505158 vs. C loss: 0.18851925598250496\n",
      "Group 50, Epoch 5: G loss: 0.24120633772441322 vs. C loss: 0.1664030369785097\n",
      "Group 50, Epoch 6: G loss: 0.5182587964194162 vs. C loss: 0.07595875610907872\n",
      "Group 51, Epoch 1: G loss: 0.4310849343027387 vs. C loss: 0.23834978375169966\n",
      "Group 51, Epoch 2: G loss: 0.25677655041217806 vs. C loss: 0.20175697571701476\n",
      "Group 51, Epoch 3: G loss: 0.11209981335060937 vs. C loss: 0.18499589463075003\n",
      "Group 51, Epoch 4: G loss: 0.12999598234891893 vs. C loss: 0.16216093881262672\n",
      "Group 51, Epoch 5: G loss: 0.31372698971203394 vs. C loss: 0.11985348992877536\n",
      "Group 51, Epoch 6: G loss: 0.5664269762379782 vs. C loss: 0.10811475995514129\n",
      "Group 52, Epoch 1: G loss: 0.4876255997589656 vs. C loss: 0.249719093243281\n",
      "Group 52, Epoch 2: G loss: 0.2989955684968404 vs. C loss: 0.2025034493870205\n",
      "Group 52, Epoch 3: G loss: 0.13713698727743967 vs. C loss: 0.15369332664542726\n",
      "Group 52, Epoch 4: G loss: 0.49166104538100097 vs. C loss: 0.1055407499273618\n",
      "Group 52, Epoch 5: G loss: 0.45683514305523465 vs. C loss: 0.07999055915408664\n",
      "Group 52, Epoch 6: G loss: 0.7954914024897983 vs. C loss: 0.04490142294930088\n",
      "Group 52, Epoch 7: G loss: 0.5328254435743603 vs. C loss: 0.07870891845474641\n",
      "Group 52, Epoch 8: G loss: 1.0050129771232605 vs. C loss: 0.019386022610382903\n",
      "Group 52, Epoch 9: G loss: 0.9151758585657392 vs. C loss: 0.022627777782165345\n",
      "Group 52, Epoch 10: G loss: 0.44381934234074183 vs. C loss: 0.21601339926322302\n",
      "Group 53, Epoch 1: G loss: 0.43231640883854455 vs. C loss: 0.24353691438833872\n",
      "Group 53, Epoch 2: G loss: 0.29699894062110355 vs. C loss: 0.1801575140820609\n",
      "Group 53, Epoch 3: G loss: 0.2559059292078018 vs. C loss: 0.12809670509563553\n",
      "Group 53, Epoch 4: G loss: 0.5998617351055145 vs. C loss: 0.12330003745026058\n",
      "Group 53, Epoch 5: G loss: 0.6056458881923131 vs. C loss: 0.05320512275728914\n",
      "Group 53, Epoch 6: G loss: 0.3967811846307346 vs. C loss: 0.2105233487155702\n",
      "Group 53, Epoch 7: G loss: 0.030519189898456842 vs. C loss: 0.20624431760774717\n",
      "Group 53, Epoch 8: G loss: 0.033544191079480305 vs. C loss: 0.2061339302195443\n",
      "Group 53, Epoch 9: G loss: 0.03288358556372779 vs. C loss: 0.20404006706343755\n",
      "Group 54, Epoch 1: G loss: 0.46472596270697464 vs. C loss: 0.2650638371706009\n",
      "Group 54, Epoch 2: G loss: 0.3760397740772791 vs. C loss: 0.18842433558570013\n",
      "Group 54, Epoch 3: G loss: 0.3071277154343469 vs. C loss: 0.1263249624106619\n",
      "Group 54, Epoch 4: G loss: 0.5942356637545995 vs. C loss: 0.11684122102128135\n",
      "Group 54, Epoch 5: G loss: 0.5310302972793579 vs. C loss: 0.22579658362600538\n",
      "Group 54, Epoch 6: G loss: 0.05881046429276466 vs. C loss: 0.20507048898273042\n",
      "Group 55, Epoch 1: G loss: 0.47678706220218114 vs. C loss: 0.2453209724691179\n",
      "Group 55, Epoch 2: G loss: 0.2872763689075197 vs. C loss: 0.18514223727915025\n",
      "Group 55, Epoch 3: G loss: 0.11770381650754384 vs. C loss: 0.15983767641915214\n",
      "Group 55, Epoch 4: G loss: 0.26611128321715766 vs. C loss: 0.17230386204189727\n",
      "Group 55, Epoch 5: G loss: 0.39503999778202603 vs. C loss: 0.13926347262329525\n",
      "Group 55, Epoch 6: G loss: 0.40431167483329783 vs. C loss: 0.20794607864485848\n",
      "Group 55, Epoch 7: G loss: 0.11048184164932796 vs. C loss: 0.19247933394379083\n",
      "Group 55, Epoch 8: G loss: 0.14903784564563205 vs. C loss: 0.14006870736678442\n",
      "Group 55, Epoch 9: G loss: 0.4991070832524981 vs. C loss: 0.06949375362859832\n",
      "Group 55, Epoch 10: G loss: 0.7702953781400409 vs. C loss: 0.0714642604192098\n",
      "Group 55, Epoch 11: G loss: 0.6992901461465019 vs. C loss: 0.026593358980284795\n",
      "Group 55, Epoch 12: G loss: 0.9631387659481593 vs. C loss: 0.014294087136578227\n",
      "Group 55, Epoch 13: G loss: 0.9186855537550791 vs. C loss: 0.004054220496780342\n",
      "Group 55, Epoch 14: G loss: 0.8345401474407741 vs. C loss: 0.09852181023193729\n",
      "Group 55, Epoch 15: G loss: 1.0208564622061596 vs. C loss: 0.03822881893979179\n",
      "Group 55, Epoch 16: G loss: 0.6743908430848802 vs. C loss: 0.04246538455805018\n",
      "Group 55, Epoch 17: G loss: 1.0188633782523018 vs. C loss: 0.009490201146238381\n",
      "Group 56, Epoch 1: G loss: 0.46495235136577057 vs. C loss: 0.25275157888730365\n",
      "Group 56, Epoch 2: G loss: 0.3502506528581892 vs. C loss: 0.14603706366486022\n",
      "Group 56, Epoch 3: G loss: 0.32704994806221555 vs. C loss: 0.20816034409734938\n",
      "Group 56, Epoch 4: G loss: 0.12125402029071534 vs. C loss: 0.17234856221410963\n",
      "Group 56, Epoch 5: G loss: 0.3429481608527047 vs. C loss: 0.09668185768855943\n",
      "Group 56, Epoch 6: G loss: 0.5123279269252504 vs. C loss: 0.0893203794128365\n",
      "Group 57, Epoch 1: G loss: 0.4722763308456966 vs. C loss: 0.2582825455400679\n",
      "Group 57, Epoch 2: G loss: 0.3224100981439863 vs. C loss: 0.176028939584891\n",
      "Group 57, Epoch 3: G loss: 0.1968867472239903 vs. C loss: 0.14052048905028236\n",
      "Group 57, Epoch 4: G loss: 0.36250361033848355 vs. C loss: 0.09348785214953953\n",
      "Group 57, Epoch 5: G loss: 0.685248303413391 vs. C loss: 0.04259247270723184\n",
      "Group 57, Epoch 6: G loss: 0.3972122286047255 vs. C loss: 0.13408754310674137\n",
      "Group 57, Epoch 7: G loss: 0.9401945097105843 vs. C loss: 0.04529192050298055\n",
      "Group 58, Epoch 1: G loss: 0.42198089446340287 vs. C loss: 0.2513583708140585\n",
      "Group 58, Epoch 2: G loss: 0.27620551373277397 vs. C loss: 0.18879743748240999\n",
      "Group 58, Epoch 3: G loss: 0.08611088267394475 vs. C loss: 0.17655793329079947\n",
      "Group 58, Epoch 4: G loss: 0.21464713045528955 vs. C loss: 0.11643359396192761\n",
      "Group 58, Epoch 5: G loss: 0.5626362809113093 vs. C loss: 0.12940393719408247\n",
      "Group 58, Epoch 6: G loss: 0.4986171441418784 vs. C loss: 0.14392413447300592\n",
      "Group 58, Epoch 7: G loss: 0.26340787964207785 vs. C loss: 0.2191020780139499\n",
      "Group 59, Epoch 1: G loss: 0.5246263955320631 vs. C loss: 0.2222812862859832\n",
      "Group 59, Epoch 2: G loss: 0.4323657010282789 vs. C loss: 0.18019222219785055\n",
      "Group 59, Epoch 3: G loss: 0.1700133021388735 vs. C loss: 0.12677756365802553\n",
      "Group 59, Epoch 4: G loss: 0.6847866041319711 vs. C loss: 0.040451238966650434\n",
      "Group 59, Epoch 5: G loss: 0.9103214979171753 vs. C loss: 0.027180143528514434\n",
      "Group 59, Epoch 6: G loss: 0.5686237047825541 vs. C loss: 0.20025201472971174\n",
      "Group 59, Epoch 7: G loss: 0.059213413617440626 vs. C loss: 0.20832163757748076\n",
      "Group 60, Epoch 1: G loss: 0.4699878820351192 vs. C loss: 0.25367679529719883\n",
      "Group 60, Epoch 2: G loss: 0.3712783455848694 vs. C loss: 0.1693203863170412\n",
      "Group 60, Epoch 3: G loss: 0.21078813161168777 vs. C loss: 0.1308057639333937\n",
      "Group 60, Epoch 4: G loss: 0.5229049708162036 vs. C loss: 0.08960280567407608\n",
      "Group 60, Epoch 5: G loss: 0.22291381316525594 vs. C loss: 0.20891502830717298\n",
      "Group 60, Epoch 6: G loss: 0.03599847603057112 vs. C loss: 0.20627855675088033\n",
      "Group 60, Epoch 7: G loss: 0.04318798077957971 vs. C loss: 0.20679067820310593\n",
      "Group 60, Epoch 8: G loss: 0.040889983943530495 vs. C loss: 0.20519488553206125\n",
      "Group 61, Epoch 1: G loss: 0.476245949949537 vs. C loss: 0.24972645772827995\n",
      "Group 61, Epoch 2: G loss: 0.3885683408805302 vs. C loss: 0.13952439526716867\n",
      "Group 61, Epoch 3: G loss: 0.3636678410427911 vs. C loss: 0.1263612988922331\n",
      "Group 61, Epoch 4: G loss: 0.6958040697234018 vs. C loss: 0.07145301500956218\n",
      "Group 61, Epoch 5: G loss: 0.2124084509909153 vs. C loss: 0.2049293584293789\n",
      "Group 61, Epoch 6: G loss: 0.015208401038710559 vs. C loss: 0.20650755531258055\n",
      "Group 62, Epoch 1: G loss: 0.44074190258979795 vs. C loss: 0.26253200074036914\n",
      "Group 62, Epoch 2: G loss: 0.30651324902262006 vs. C loss: 0.16583528618017832\n",
      "Group 62, Epoch 3: G loss: 0.22304724582604 vs. C loss: 0.16982924358712304\n",
      "Group 62, Epoch 4: G loss: 0.3655252609934126 vs. C loss: 0.15293209917015502\n",
      "Group 62, Epoch 5: G loss: 0.35822802569184986 vs. C loss: 0.16957523425420126\n",
      "Group 62, Epoch 6: G loss: 0.22811477099146163 vs. C loss: 0.24352618555227915\n",
      "Group 62, Epoch 7: G loss: 0.050310948916843956 vs. C loss: 0.2074769851234224\n",
      "Group 63, Epoch 1: G loss: 0.48518682462828505 vs. C loss: 0.26156368686093223\n",
      "Group 63, Epoch 2: G loss: 0.3010748088359833 vs. C loss: 0.17327808837095895\n",
      "Group 63, Epoch 3: G loss: 0.27803377679416114 vs. C loss: 0.11174172990851933\n",
      "Group 63, Epoch 4: G loss: 0.6347521313599177 vs. C loss: 0.0977977423204316\n",
      "Group 63, Epoch 5: G loss: 0.35096803222383766 vs. C loss: 0.1929373600416713\n",
      "Group 63, Epoch 6: G loss: 0.2548166275024414 vs. C loss: 0.09015097096562386\n",
      "Group 64, Epoch 1: G loss: 0.47824731043406893 vs. C loss: 0.2462293671237098\n",
      "Group 64, Epoch 2: G loss: 0.24105241468974523 vs. C loss: 0.1895259221394857\n",
      "Group 64, Epoch 3: G loss: 0.1200775825551578 vs. C loss: 0.16139207945929634\n",
      "Group 64, Epoch 4: G loss: 0.28840521403721403 vs. C loss: 0.13339009632666907\n",
      "Group 64, Epoch 5: G loss: 0.5081441317285811 vs. C loss: 0.07237587620814641\n",
      "Group 64, Epoch 6: G loss: 0.28648254935230527 vs. C loss: 0.19863651278946134\n",
      "Group 64, Epoch 7: G loss: 0.14304111855370658 vs. C loss: 0.18398114873303306\n",
      "Group 64, Epoch 8: G loss: 0.6900697861398969 vs. C loss: 0.11031344450182384\n",
      "Group 64, Epoch 9: G loss: 0.6902126465524946 vs. C loss: 0.02746047317567799\n",
      "Group 64, Epoch 10: G loss: 0.7766045706612722 vs. C loss: 0.03236731835123566\n",
      "Group 64, Epoch 11: G loss: 0.7977377653121949 vs. C loss: 0.037532574729993946\n",
      "Group 64, Epoch 12: G loss: 1.0121150016784668 vs. C loss: 0.014090014414654838\n",
      "Group 64, Epoch 13: G loss: 1.0293587957109724 vs. C loss: 0.024109049708284952\n",
      "Group 64, Epoch 14: G loss: 0.9772764103753226 vs. C loss: 0.005145345892136295\n",
      "Group 64, Epoch 15: G loss: 1.012991292136056 vs. C loss: 0.0041505083952668225\n",
      "Group 64, Epoch 16: G loss: 0.9719357184001378 vs. C loss: 0.0017565987606455264\n",
      "Group 64, Epoch 17: G loss: 1.0353577068873814 vs. C loss: 6.759699721846523e-05\n",
      "Group 64, Epoch 18: G loss: 1.0534616334097726 vs. C loss: 0.009255702409265924\n",
      "Group 64, Epoch 19: G loss: 0.735094015938895 vs. C loss: 0.05426741242990829\n",
      "Group 64, Epoch 20: G loss: 1.0532648290906634 vs. C loss: 0.02333230409324945\n",
      "Group 64, Epoch 21: G loss: 0.9659135035106113 vs. C loss: 0.003713752880382042\n",
      "Group 64, Epoch 22: G loss: 1.0314743144171579 vs. C loss: 0.0001326313245550005\n",
      "Group 64, Epoch 23: G loss: 1.0425568887165615 vs. C loss: 0.003143053723559003\n",
      "Group 64, Epoch 24: G loss: 1.0361557177134924 vs. C loss: 0.006367391358556536\n",
      "Group 64, Epoch 25: G loss: 0.37396530977317266 vs. C loss: 0.20534312228361765\n",
      "Group 65, Epoch 1: G loss: 0.45669958080564227 vs. C loss: 0.23590313063727486\n",
      "Group 65, Epoch 2: G loss: 0.40490294269153054 vs. C loss: 0.1407332428627544\n",
      "Group 65, Epoch 3: G loss: 0.4088571850742612 vs. C loss: 0.19845559861924914\n",
      "Group 65, Epoch 4: G loss: 0.24716144417013441 vs. C loss: 0.1471608661943012\n",
      "Group 65, Epoch 5: G loss: 0.7134019511086599 vs. C loss: 0.02793721875382794\n",
      "Group 65, Epoch 6: G loss: 0.8680959173611232 vs. C loss: 0.024384861398074362\n",
      "Group 66, Epoch 1: G loss: 0.45212038585117886 vs. C loss: 0.24827177657021415\n",
      "Group 66, Epoch 2: G loss: 0.30450311345713477 vs. C loss: 0.2093328750795788\n",
      "Group 66, Epoch 3: G loss: 0.08124656464372362 vs. C loss: 0.19123885449435976\n",
      "Group 66, Epoch 4: G loss: 0.10935113919632776 vs. C loss: 0.15283560587300196\n",
      "Group 66, Epoch 5: G loss: 0.49366291846547805 vs. C loss: 0.09860165417194366\n",
      "Group 66, Epoch 6: G loss: 0.4784789851733617 vs. C loss: 0.08966822591092853\n",
      "Group 66, Epoch 7: G loss: 0.8204722438539778 vs. C loss: 0.027031370852556497\n",
      "Group 66, Epoch 8: G loss: 0.8337847760745457 vs. C loss: 0.0222533465259605\n",
      "Group 66, Epoch 9: G loss: 0.8916227255548749 vs. C loss: 0.008041145875015194\n",
      "Group 66, Epoch 10: G loss: 0.9679913418633598 vs. C loss: 0.005919323428922023\n",
      "Group 66, Epoch 11: G loss: 0.5132683358022144 vs. C loss: 0.20824603570832148\n",
      "Group 66, Epoch 12: G loss: 0.0546054978455816 vs. C loss: 0.21329507893986174\n",
      "Group 66, Epoch 13: G loss: 0.050897616254431864 vs. C loss: 0.20827206182810995\n",
      "Group 66, Epoch 14: G loss: 0.049868187308311454 vs. C loss: 0.2082851794030931\n",
      "Group 66, Epoch 15: G loss: 0.049445738962718415 vs. C loss: 0.2082865639693207\n",
      "Group 66, Epoch 16: G loss: 0.04861910449607032 vs. C loss: 0.20828728046682146\n",
      "Group 66, Epoch 17: G loss: 0.04430227652192116 vs. C loss: 0.20829223924212983\n",
      "Group 67, Epoch 1: G loss: 0.4567863719803947 vs. C loss: 0.25711116856998867\n",
      "Group 67, Epoch 2: G loss: 0.34783901997974936 vs. C loss: 0.15069714271359974\n",
      "Group 67, Epoch 3: G loss: 0.4115442144019263 vs. C loss: 0.16565966275003222\n",
      "Group 67, Epoch 4: G loss: 0.5206609981400626 vs. C loss: 0.07261476769215532\n",
      "Group 67, Epoch 5: G loss: 0.373331070797784 vs. C loss: 0.1509607293539577\n",
      "Group 67, Epoch 6: G loss: 0.6359698687280927 vs. C loss: 0.12291939080589348\n",
      "Group 67, Epoch 7: G loss: 0.32275506768907825 vs. C loss: 0.06860778128935231\n",
      "Group 67, Epoch 8: G loss: 0.946674290725163 vs. C loss: 0.011107463441375228\n",
      "Group 67, Epoch 9: G loss: 0.9756589889526367 vs. C loss: 0.010449183018257221\n",
      "Group 67, Epoch 10: G loss: 0.9662371243749346 vs. C loss: 0.0028234131815325883\n",
      "Group 67, Epoch 11: G loss: 0.8753093361854554 vs. C loss: 0.006508243283153407\n",
      "Group 67, Epoch 12: G loss: 0.9623980147497994 vs. C loss: 0.0006849255555102396\n",
      "Group 67, Epoch 13: G loss: 1.0168921232223511 vs. C loss: 7.312578115185413e-05\n",
      "Group 67, Epoch 14: G loss: 1.02594051361084 vs. C loss: 2.4653098888746976e-05\n",
      "Group 67, Epoch 15: G loss: 1.028378817013332 vs. C loss: 8.867652933177952e-05\n",
      "Group 67, Epoch 16: G loss: 1.029309848376683 vs. C loss: 0.017885681508838996\n",
      "Group 67, Epoch 17: G loss: 1.0277769088745117 vs. C loss: 2.8170738611758377e-05\n",
      "Group 67, Epoch 18: G loss: 1.0241188185555594 vs. C loss: 3.1481748818704444e-05\n",
      "Group 67, Epoch 19: G loss: 1.0209570407867432 vs. C loss: 4.618707316694781e-05\n",
      "Group 67, Epoch 20: G loss: 1.0189627272742134 vs. C loss: 0.00011687867501879938\n",
      "Group 67, Epoch 21: G loss: 1.0178160565240042 vs. C loss: 0.0003837816572437684\n",
      "Group 67, Epoch 22: G loss: 1.013705975668771 vs. C loss: 0.0001069668044995827\n",
      "Group 67, Epoch 23: G loss: 1.0099024772644043 vs. C loss: 0.0036924559454847537\n",
      "Group 67, Epoch 24: G loss: 0.9939153109277996 vs. C loss: 0.000142210753943396\n",
      "Group 67, Epoch 25: G loss: 1.027103100504194 vs. C loss: 1.0442820186856099e-05\n",
      "Group 68, Epoch 1: G loss: 0.4939165651798248 vs. C loss: 0.24084512889385223\n",
      "Group 68, Epoch 2: G loss: 0.36345321195466174 vs. C loss: 0.19533804804086685\n",
      "Group 68, Epoch 3: G loss: 0.10994145657335011 vs. C loss: 0.17767901221911112\n",
      "Group 68, Epoch 4: G loss: 0.1764756675277437 vs. C loss: 0.12833011067575878\n",
      "Group 68, Epoch 5: G loss: 0.5597499081066676 vs. C loss: 0.07314594007200666\n",
      "Group 68, Epoch 6: G loss: 0.7495809708322797 vs. C loss: 0.02444441502706872\n",
      "Group 69, Epoch 1: G loss: 0.5029905200004577 vs. C loss: 0.2502831336524752\n",
      "Group 69, Epoch 2: G loss: 0.3075101149933679 vs. C loss: 0.18943716751204598\n",
      "Group 69, Epoch 3: G loss: 0.12500226157052177 vs. C loss: 0.16135935650931466\n",
      "Group 69, Epoch 4: G loss: 0.3738783052989415 vs. C loss: 0.13302920758724215\n",
      "Group 69, Epoch 5: G loss: 0.5314303926059177 vs. C loss: 0.06136914756562974\n",
      "Group 69, Epoch 6: G loss: 0.7419631140572686 vs. C loss: 0.024645515717566013\n",
      "Group 70, Epoch 1: G loss: 0.48044715353420797 vs. C loss: 0.24931361940171984\n",
      "Group 70, Epoch 2: G loss: 0.29400070777961185 vs. C loss: 0.17108218040731216\n",
      "Group 70, Epoch 3: G loss: 0.2534075605017798 vs. C loss: 0.14942394776476756\n",
      "Group 70, Epoch 4: G loss: 0.5116703280380793 vs. C loss: 0.15835797124438813\n",
      "Group 70, Epoch 5: G loss: 0.3624047296387808 vs. C loss: 0.17102688882086015\n",
      "Group 70, Epoch 6: G loss: 0.22532769239374567 vs. C loss: 0.20106140938070086\n",
      "Group 71, Epoch 1: G loss: 0.44042121938296735 vs. C loss: 0.2523824307653639\n",
      "Group 71, Epoch 2: G loss: 0.40460869669914246 vs. C loss: 0.1439855918288231\n",
      "Group 71, Epoch 3: G loss: 0.40558868476322724 vs. C loss: 0.12450654970275031\n",
      "Group 71, Epoch 4: G loss: 0.6903919424329485 vs. C loss: 0.044263103769885175\n",
      "Group 71, Epoch 5: G loss: 0.5080571153334208 vs. C loss: 0.20146659637490907\n",
      "Group 71, Epoch 6: G loss: 0.03623264751264027 vs. C loss: 0.20670789314640892\n",
      "Group 72, Epoch 1: G loss: 0.45790738548551285 vs. C loss: 0.2501955247587628\n",
      "Group 72, Epoch 2: G loss: 0.30226826838084625 vs. C loss: 0.17021205379731127\n",
      "Group 72, Epoch 3: G loss: 0.2245707712003163 vs. C loss: 0.1459853251775106\n",
      "Group 72, Epoch 4: G loss: 0.2555477568081447 vs. C loss: 0.2267939398686091\n",
      "Group 72, Epoch 5: G loss: 0.0765322425535747 vs. C loss: 0.1983371070689625\n",
      "Group 72, Epoch 6: G loss: 0.07181173648153034 vs. C loss: 0.17860130551788544\n",
      "Group 73, Epoch 1: G loss: 0.4686692067555019 vs. C loss: 0.24475028779771593\n",
      "Group 73, Epoch 2: G loss: 0.3798747147832598 vs. C loss: 0.17037492328219941\n",
      "Group 73, Epoch 3: G loss: 0.2621984213590622 vs. C loss: 0.15797929465770721\n",
      "Group 73, Epoch 4: G loss: 0.33507762423583437 vs. C loss: 0.16281658907731372\n",
      "Group 73, Epoch 5: G loss: 0.456280584846224 vs. C loss: 0.10152357361382909\n",
      "Group 73, Epoch 6: G loss: 0.6701981340135849 vs. C loss: 0.04122218365470569\n",
      "Group 74, Epoch 1: G loss: 0.46961414303098403 vs. C loss: 0.24369090961085427\n",
      "Group 74, Epoch 2: G loss: 0.338239717057773 vs. C loss: 0.182951549688975\n",
      "Group 74, Epoch 3: G loss: 0.14669264299528942 vs. C loss: 0.15903921176989874\n",
      "Group 74, Epoch 4: G loss: 0.3379561160291944 vs. C loss: 0.15878848234812418\n",
      "Group 74, Epoch 5: G loss: 0.4581416036401476 vs. C loss: 0.19860899613963234\n",
      "Group 74, Epoch 6: G loss: 0.12652360158307213 vs. C loss: 0.20872711638609567\n",
      "Group 74, Epoch 7: G loss: 0.07306963418211256 vs. C loss: 0.1854694535334905\n",
      "Group 75, Epoch 1: G loss: 0.4898411316531046 vs. C loss: 0.2562155011627409\n",
      "Group 75, Epoch 2: G loss: 0.5657894424029759 vs. C loss: 0.09767692867252563\n",
      "Group 75, Epoch 3: G loss: 0.4390933998993466 vs. C loss: 0.1877854404350122\n",
      "Group 75, Epoch 4: G loss: 0.1180482198085104 vs. C loss: 0.1474461480975151\n",
      "Group 75, Epoch 5: G loss: 0.5940216984067643 vs. C loss: 0.0425444059073925\n",
      "Group 75, Epoch 6: G loss: 0.7114089769976479 vs. C loss: 0.06584925432172087\n",
      "Group 76, Epoch 1: G loss: 0.45648585557937615 vs. C loss: 0.24228129287560782\n",
      "Group 76, Epoch 2: G loss: 0.2610854463917868 vs. C loss: 0.18168277707364824\n",
      "Group 76, Epoch 3: G loss: 0.1121172457933426 vs. C loss: 0.19762116918961206\n",
      "Group 76, Epoch 4: G loss: 0.059561347748552054 vs. C loss: 0.1974612863527404\n",
      "Group 76, Epoch 5: G loss: 0.07304574187312807 vs. C loss: 0.1926336901055442\n",
      "Group 76, Epoch 6: G loss: 0.12016711618219104 vs. C loss: 0.16518441008196938\n",
      "Group 77, Epoch 1: G loss: 0.4684739172458648 vs. C loss: 0.25690675444073147\n",
      "Group 77, Epoch 2: G loss: 0.3891071234430586 vs. C loss: 0.17251151303450266\n",
      "Group 77, Epoch 3: G loss: 0.25454796637807575 vs. C loss: 0.11785529429713885\n",
      "Group 77, Epoch 4: G loss: 0.732945806639535 vs. C loss: 0.05773499939176772\n",
      "Group 77, Epoch 5: G loss: 0.5401238484042031 vs. C loss: 0.1985980255736245\n",
      "Group 77, Epoch 6: G loss: 0.05783338365810258 vs. C loss: 0.19690334465768602\n",
      "Group 78, Epoch 1: G loss: 0.45633212498256137 vs. C loss: 0.25719672772619456\n",
      "Group 78, Epoch 2: G loss: 0.36007364647729057 vs. C loss: 0.16286161210801867\n",
      "Group 78, Epoch 3: G loss: 0.34826212184769767 vs. C loss: 0.10829466415776147\n",
      "Group 78, Epoch 4: G loss: 0.5889723317963736 vs. C loss: 0.06262634901536836\n",
      "Group 78, Epoch 5: G loss: 0.34396441131830224 vs. C loss: 0.2131933073202769\n",
      "Group 78, Epoch 6: G loss: 0.047113118107829775 vs. C loss: 0.20696161852942574\n",
      "Group 79, Epoch 1: G loss: 0.5230009496212006 vs. C loss: 0.23397424485948348\n",
      "Group 79, Epoch 2: G loss: 0.37995390721729827 vs. C loss: 0.15154277947213915\n",
      "Group 79, Epoch 3: G loss: 0.27816936118262153 vs. C loss: 0.15876706027322346\n",
      "Group 79, Epoch 4: G loss: 0.5571883848735263 vs. C loss: 0.0724147798286544\n",
      "Group 79, Epoch 5: G loss: 0.5971384295395442 vs. C loss: 0.07151916581723425\n",
      "Group 79, Epoch 6: G loss: 0.48913300122533526 vs. C loss: 0.07018014560970995\n",
      "Group 80, Epoch 1: G loss: 0.43483099596840996 vs. C loss: 0.2537004186047448\n",
      "Group 80, Epoch 2: G loss: 0.3347264251538685 vs. C loss: 0.1737842179007\n",
      "Group 80, Epoch 3: G loss: 0.20794118025473188 vs. C loss: 0.1792638020382987\n",
      "Group 80, Epoch 4: G loss: 0.2933304067168917 vs. C loss: 0.11586616436640422\n",
      "Group 80, Epoch 5: G loss: 0.6985226767403738 vs. C loss: 0.06533269501394695\n",
      "Group 80, Epoch 6: G loss: 0.49108457139560163 vs. C loss: 0.21223566432793936\n",
      "Group 81, Epoch 1: G loss: 0.4432901016303471 vs. C loss: 0.25283024542861515\n",
      "Group 81, Epoch 2: G loss: 0.30763559341430674 vs. C loss: 0.16034729695982405\n",
      "Group 81, Epoch 3: G loss: 0.23592782957213268 vs. C loss: 0.1130906616648038\n",
      "Group 81, Epoch 4: G loss: 0.5470003936971937 vs. C loss: 0.085780863960584\n",
      "Group 81, Epoch 5: G loss: 0.7725271769932338 vs. C loss: 0.06943515688180925\n",
      "Group 81, Epoch 6: G loss: 0.2754234419337341 vs. C loss: 0.2005522321495745\n",
      "Group 82, Epoch 1: G loss: 0.4689509102276394 vs. C loss: 0.23957541584968567\n",
      "Group 82, Epoch 2: G loss: 0.4925107598304748 vs. C loss: 0.1127192767130004\n",
      "Group 82, Epoch 3: G loss: 0.28276191162211556 vs. C loss: 0.1999846233261956\n",
      "Group 82, Epoch 4: G loss: 0.08019926569291523 vs. C loss: 0.14503257473309836\n",
      "Group 82, Epoch 5: G loss: 0.5920264993395125 vs. C loss: 0.10140817198488446\n",
      "Group 82, Epoch 6: G loss: 0.5586368654455457 vs. C loss: 0.050277659462557904\n",
      "Group 83, Epoch 1: G loss: 0.42633883016450064 vs. C loss: 0.25879025128152633\n",
      "Group 83, Epoch 2: G loss: 0.23452156271253313 vs. C loss: 0.18451109859678483\n",
      "Group 83, Epoch 3: G loss: 0.08722111561468669 vs. C loss: 0.18053186933199564\n",
      "Group 83, Epoch 4: G loss: 0.1896309537546975 vs. C loss: 0.15606982509295145\n",
      "Group 83, Epoch 5: G loss: 0.33637439608573916 vs. C loss: 0.16066942612330118\n",
      "Group 83, Epoch 6: G loss: 0.23931962507111687 vs. C loss: 0.1377233995331658\n",
      "Group 84, Epoch 1: G loss: 0.4464716391904014 vs. C loss: 0.24310553239451516\n",
      "Group 84, Epoch 2: G loss: 0.26700118439538134 vs. C loss: 0.1857003209491571\n",
      "Group 84, Epoch 3: G loss: 0.25106200873851775 vs. C loss: 0.17476243442959258\n",
      "Group 84, Epoch 4: G loss: 0.15039493548018593 vs. C loss: 0.21049702167510986\n",
      "Group 84, Epoch 5: G loss: 0.0649460959647383 vs. C loss: 0.20863806787464353\n",
      "Group 84, Epoch 6: G loss: 0.029402387461491993 vs. C loss: 0.20295314159658218\n",
      "Group 85, Epoch 1: G loss: 0.4296372881957463 vs. C loss: 0.2523039049572415\n",
      "Group 85, Epoch 2: G loss: 0.2922045980181012 vs. C loss: 0.1607898680700196\n",
      "Group 85, Epoch 3: G loss: 0.22665924259594508 vs. C loss: 0.16086015022463268\n",
      "Group 85, Epoch 4: G loss: 0.39111537890774867 vs. C loss: 0.16231196208132637\n",
      "Group 85, Epoch 5: G loss: 0.24683023520878386 vs. C loss: 0.20213027215666243\n",
      "Group 85, Epoch 6: G loss: 0.12725909841912136 vs. C loss: 0.19174886412090728\n",
      "Group 85, Epoch 7: G loss: 0.12982656295810427 vs. C loss: 0.16302136662933564\n",
      "Group 85, Epoch 8: G loss: 0.38774264880589077 vs. C loss: 0.07996839988562796\n",
      "Group 86, Epoch 1: G loss: 0.49658591236386984 vs. C loss: 0.24655363957087198\n",
      "Group 86, Epoch 2: G loss: 0.3008496918848583 vs. C loss: 0.19045370817184448\n",
      "Group 86, Epoch 3: G loss: 0.10347113481589727 vs. C loss: 0.15850425097677442\n",
      "Group 86, Epoch 4: G loss: 0.32588351794651577 vs. C loss: 0.11244646749562687\n",
      "Group 86, Epoch 5: G loss: 0.6299125415938241 vs. C loss: 0.05610646886958016\n",
      "Group 86, Epoch 6: G loss: 0.603742822578975 vs. C loss: 0.03322366324977742\n",
      "Group 87, Epoch 1: G loss: 0.42106001888002664 vs. C loss: 0.27359072036213344\n",
      "Group 87, Epoch 2: G loss: 0.270120108127594 vs. C loss: 0.18634918828805289\n",
      "Group 87, Epoch 3: G loss: 0.1942718842199871 vs. C loss: 0.1498013354010052\n",
      "Group 87, Epoch 4: G loss: 0.5941821055752889 vs. C loss: 0.1198653562201394\n",
      "Group 87, Epoch 5: G loss: 0.5347459145954677 vs. C loss: 0.06403027806017135\n",
      "Group 87, Epoch 6: G loss: 0.6906408952815192 vs. C loss: 0.11239933553669189\n",
      "Group 87, Epoch 7: G loss: 0.4030515302504812 vs. C loss: 0.24047585825125375\n",
      "Group 88, Epoch 1: G loss: 0.46814913579395834 vs. C loss: 0.24704518500301573\n",
      "Group 88, Epoch 2: G loss: 0.36637374332972933 vs. C loss: 0.1946824855274624\n",
      "Group 88, Epoch 3: G loss: 0.12722065044300895 vs. C loss: 0.1917272677852048\n",
      "Group 88, Epoch 4: G loss: 0.08831913002899716 vs. C loss: 0.17963210244973501\n",
      "Group 88, Epoch 5: G loss: 0.14347078800201415 vs. C loss: 0.1874963790178299\n",
      "Group 88, Epoch 6: G loss: 0.24156874205384934 vs. C loss: 0.19444669451978472\n",
      "Group 88, Epoch 7: G loss: 0.1597435725586755 vs. C loss: 0.17698043750392065\n",
      "Group 88, Epoch 8: G loss: 0.18661718325955529 vs. C loss: 0.1483549401164055\n",
      "Group 89, Epoch 1: G loss: 0.4493829182216099 vs. C loss: 0.2784150176578098\n",
      "Group 89, Epoch 2: G loss: 0.38632082343101504 vs. C loss: 0.17828640507327187\n",
      "Group 89, Epoch 3: G loss: 0.23492109179496762 vs. C loss: 0.16135360962814757\n",
      "Group 89, Epoch 4: G loss: 0.27223633272307257 vs. C loss: 0.1784852221608162\n",
      "Group 89, Epoch 5: G loss: 0.24781431981495447 vs. C loss: 0.20343029499053955\n",
      "Group 89, Epoch 6: G loss: 0.08079634585550853 vs. C loss: 0.19341946310467192\n",
      "Group 90, Epoch 1: G loss: 0.45039568798882623 vs. C loss: 0.24731735554006362\n",
      "Group 90, Epoch 2: G loss: 0.3372060639517648 vs. C loss: 0.18854699035485586\n",
      "Group 90, Epoch 3: G loss: 0.19302664697170258 vs. C loss: 0.16835208402739632\n",
      "Group 90, Epoch 4: G loss: 0.25296310654708315 vs. C loss: 0.1212191383043925\n",
      "Group 90, Epoch 5: G loss: 0.5619253941944667 vs. C loss: 0.0666906630827321\n",
      "Group 90, Epoch 6: G loss: 0.3416250697204045 vs. C loss: 0.1502907243039873\n",
      "Group 90, Epoch 7: G loss: 0.5013350878443037 vs. C loss: 0.21376292076375747\n",
      "Group 91, Epoch 1: G loss: 0.45002065726688933 vs. C loss: 0.25197875830862254\n",
      "Group 91, Epoch 2: G loss: 0.33865275723593574 vs. C loss: 0.1969963428046968\n",
      "Group 91, Epoch 3: G loss: 0.11353312134742735 vs. C loss: 0.2003015594349967\n",
      "Group 91, Epoch 4: G loss: 0.07547420518738883 vs. C loss: 0.18708065731657875\n",
      "Group 91, Epoch 5: G loss: 0.17415270592485155 vs. C loss: 0.1371315552128686\n",
      "Group 91, Epoch 6: G loss: 0.5381683332579477 vs. C loss: 0.0860276073217392\n",
      "Group 92, Epoch 1: G loss: 0.46143100772585194 vs. C loss: 0.2531922211249669\n",
      "Group 92, Epoch 2: G loss: 0.35785714983940126 vs. C loss: 0.17667936409513155\n",
      "Group 92, Epoch 3: G loss: 0.2782312435763223 vs. C loss: 0.07934932534893353\n",
      "Group 92, Epoch 4: G loss: 0.7124941540615899 vs. C loss: 0.1698182432187928\n",
      "Group 92, Epoch 5: G loss: 0.6048655859061649 vs. C loss: 0.09493369443549049\n",
      "Group 92, Epoch 6: G loss: 0.9061297723225185 vs. C loss: 0.011571068213217788\n",
      "Group 93, Epoch 1: G loss: 0.467805392401559 vs. C loss: 0.24700198405318788\n",
      "Group 93, Epoch 2: G loss: 0.3543944520609719 vs. C loss: 0.15284537689553368\n",
      "Group 93, Epoch 3: G loss: 0.375978969676154 vs. C loss: 0.1330127240055137\n",
      "Group 93, Epoch 4: G loss: 0.4621589311531612 vs. C loss: 0.12462667210234535\n",
      "Group 93, Epoch 5: G loss: 0.6720503313200814 vs. C loss: 0.035664849604169525\n",
      "Group 93, Epoch 6: G loss: 0.8356963208743504 vs. C loss: 0.012576908235334689\n",
      "Group 93, Epoch 7: G loss: 0.9339056713240488 vs. C loss: 0.009354029523415698\n",
      "Group 93, Epoch 8: G loss: 0.5075942414147513 vs. C loss: 0.08501313916511005\n",
      "Group 93, Epoch 9: G loss: 0.9215431145259313 vs. C loss: 0.03025233844527975\n",
      "Group 93, Epoch 10: G loss: 0.39607548303902146 vs. C loss: 0.20733105018734932\n",
      "Group 93, Epoch 11: G loss: 0.0297822892665863 vs. C loss: 0.20776293840673235\n",
      "Group 93, Epoch 12: G loss: 0.03283503577113152 vs. C loss: 0.20760764429966608\n",
      "Group 93, Epoch 13: G loss: 0.02795985671026366 vs. C loss: 0.2047574818134308\n",
      "Group 94, Epoch 1: G loss: 0.5197273629052298 vs. C loss: 0.24906381302409705\n",
      "Group 94, Epoch 2: G loss: 0.31649626578603474 vs. C loss: 0.1808879491355684\n",
      "Group 94, Epoch 3: G loss: 0.22820300161838533 vs. C loss: 0.1309600327577856\n",
      "Group 94, Epoch 4: G loss: 0.4799838304519654 vs. C loss: 0.08072338832749261\n",
      "Group 94, Epoch 5: G loss: 0.6086638535772051 vs. C loss: 0.0542956677575906\n",
      "Group 94, Epoch 6: G loss: 0.624759497812816 vs. C loss: 0.06601828688548671\n",
      "Group 95, Epoch 1: G loss: 0.4551666957991464 vs. C loss: 0.24165766768985322\n",
      "Group 95, Epoch 2: G loss: 0.2946456376995359 vs. C loss: 0.194149492515458\n",
      "Group 95, Epoch 3: G loss: 0.09536352625914982 vs. C loss: 0.19425598118040296\n",
      "Group 95, Epoch 4: G loss: 0.08283034509846143 vs. C loss: 0.18469474216302237\n",
      "Group 95, Epoch 5: G loss: 0.18067294572080886 vs. C loss: 0.13678727133406532\n",
      "Group 95, Epoch 6: G loss: 0.4220437739576612 vs. C loss: 0.09718714820014106\n"
     ]
    }
   ],
   "source": [
    "loss_ma = [90, 90, 90];\n",
    "extvar = {\"begin\": 10};\n",
    "\n",
    "def plot_current_map(inputs):\n",
    "    # plot it each epoch\n",
    "    mp = construct_map_with_sliders(inputs, extvar=extvar);\n",
    "    # to make it clearer, add the start pos\n",
    "    npa = np.concatenate([[np.concatenate([extvar[\"start_pos\"] / np.array([512, 384]), [0, 0]])], tf.stack(mp).numpy().squeeze()])\n",
    "    fig, ax = plt.subplots()\n",
    "    x, y = np.transpose(npa)[0:2]\n",
    "    #x, y = np.random.rand(2, 20)\n",
    "    line = MyLine(x, y, mfc='red', ms=12)\n",
    "    line.text.set_color('red')\n",
    "    line.text.set_fontsize(16)\n",
    "    ax.add_line(line)\n",
    "    plt.show()\n",
    "\n",
    "def generative_model(in_params, out_params, loss_func='mse'):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, input_shape=(in_params,)),# activation=tf.nn.elu, input_shape=(train_data.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(128, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(out_params, activation=tf.nn.tanh)#,\n",
    "#         keras.layers.Lambda(lambda x: (x+1)/2, output_shape=(out_params,))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.002) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.002) #Adamoptimizer?\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return model\n",
    "\n",
    "def mixed_model(generator, mapping_layer, discriminator, in_params):\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    inp = keras.layers.Input(shape=(in_params,))\n",
    "    start_pos = keras.layers.Input(shape = (2,))#tf.convert_to_tensor([0, 0], dtype=tf.float32)\n",
    "    rel = keras.layers.Input(shape = (7, note_group_size))#tf.zeros((5, note_group_size), dtype=tf.float32)\n",
    "    interm1 = generator(inp)\n",
    "    interm2 = mapping_layer(interm1)\n",
    "    end = discriminator(interm2)\n",
    "    model = keras.Model(inputs = inp, outputs = [interm1, interm2, end])\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.optimizers.Adam(0.001) #Adamoptimizer?\n",
    "    except:\n",
    "        optimizer = tf.train.AdamOptimizer(0.001) #Adamoptimizer?\n",
    "        \n",
    "    losses = [AlwaysZeroCustomLoss(), BoxCustomLoss(), GenerativeCustomLoss()];\n",
    "\n",
    "    model.compile(loss=losses,\n",
    "                  loss_weights=[1e-8, 1, 1],\n",
    "                optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def conv_input(inp, extvar):\n",
    "#     Now it only uses single input\n",
    "    return inp;\n",
    "\n",
    "\n",
    "plot_noise = np.random.random((1, GAN_PARAMS[\"g_input_size\"]));\n",
    "\n",
    "# Pre-fit classifier for 1 epoch\n",
    "# history = classifier_model.fit(actual_train_data, actual_train_labels, epochs=1,\n",
    "#                     validation_split=0.2, verbose=0,\n",
    "#                     callbacks=[])\n",
    "\n",
    "# build models first, then train (it is faster in TF 2.0)\n",
    "\n",
    "def make_models():\n",
    "        \n",
    "    extvar[\"begin\"] = 0;\n",
    "    extvar[\"start_pos\"] = [256, 192];\n",
    "    extvar[\"length_multiplier\"] = 1;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    classifier_model = build_classifier_model();\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    \n",
    "    gmodel = generative_model(g_input_size, note_group_size * 4);\n",
    "    mapping_layer = KerasCustomMappingLayer(extvar);\n",
    "    mmodel = mixed_model(gmodel, mapping_layer, classifier_model, g_input_size);\n",
    "    \n",
    "    default_weights = mmodel.get_weights();\n",
    "    \n",
    "    return gmodel, mapping_layer, classifier_model, mmodel, default_weights;\n",
    "\n",
    "def set_extvar(models, extvar):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    mapping_layer.set_extvar(extvar);\n",
    "    \n",
    "def reset_model_weights(models):\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    weights = default_weights;\n",
    "    mmodel.set_weights(weights);\n",
    "\n",
    "# we can train all the classifiers first, onto Epoch X [x = 1~10]\n",
    "# then train the generators to fit to them\n",
    "# to reduce some training time.\n",
    "# but i think it doesn't work too well since it's the generator which is slow...\n",
    "\n",
    "def generate_set(models, begin = 0, start_pos=[256, 192], group_id=-1, length_multiplier=1, plot_map=True):\n",
    "    extvar[\"begin\"] = begin;\n",
    "    extvar[\"start_pos\"] = start_pos;\n",
    "    extvar[\"length_multiplier\"] = length_multiplier;\n",
    "    extvar[\"next_from_slider_end\"] = GAN_PARAMS[\"next_from_slider_end\"];\n",
    "    \n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    max_epoch = GAN_PARAMS[\"max_epoch\"];\n",
    "    good_epoch = GAN_PARAMS[\"good_epoch\"] - 1;\n",
    "    g_multiplier = GAN_PARAMS[\"g_epochs\"];\n",
    "    c_multiplier = GAN_PARAMS[\"c_epochs\"];\n",
    "    g_batch = GAN_PARAMS[\"g_batch\"];\n",
    "    g_input_size = GAN_PARAMS[\"g_input_size\"];\n",
    "    c_true_batch = GAN_PARAMS[\"c_true_batch\"];\n",
    "    c_false_batch = GAN_PARAMS[\"c_false_batch\"];\n",
    "    \n",
    "    reset_model_weights(models);\n",
    "    set_extvar(models, extvar);\n",
    "    gmodel, mapping_layer, classifier_model, mmodel, default_weights = models;\n",
    "    \n",
    "    # see the summaries\n",
    "#     gmodel.summary()\n",
    "#     classifier_model.summary()\n",
    "#     mmodel.summary()\n",
    "\n",
    "    for i in range(max_epoch):\n",
    "        \n",
    "        gnoise = np.random.random((g_batch, g_input_size));\n",
    "        glabel = [np.zeros((g_batch, note_group_size * 4)), np.ones((g_batch,)), np.ones((g_batch,))]\n",
    "        ginput = conv_input(gnoise, extvar);\n",
    "        \n",
    "        # fit mmodel instead of gmodel\n",
    "        history = mmodel.fit(ginput, glabel, epochs=g_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        pred_noise = np.random.random((c_false_batch, g_input_size));\n",
    "        pred_input = conv_input(pred_noise, extvar);\n",
    "        predicted_maps_data, predicted_maps_mapped, _predclass = mmodel.predict(pred_input);\n",
    "        new_false_maps = predicted_maps_mapped;\n",
    "        new_false_labels = np.zeros(c_false_batch);\n",
    "        \n",
    "\n",
    "        rn = np.random.randint(0, special_train_data.shape[0], (c_true_batch,))\n",
    "        actual_train_data = np.concatenate((new_false_maps, special_train_data[rn]), axis=0); #special_false_data[st:se], \n",
    "        actual_train_labels = np.concatenate((new_false_labels, special_train_labels[rn]), axis=0); #special_false_labels[st:se], \n",
    "        \n",
    "    \n",
    "        history2 = classifier_model.fit(actual_train_data, actual_train_labels, epochs=c_multiplier,\n",
    "                            validation_split=0.2, verbose=0,\n",
    "                            callbacks=[])\n",
    "        \n",
    "        # calculate the losses\n",
    "        g_loss = np.mean(history.history['loss']);\n",
    "        c_loss = np.mean(history2.history['loss']);\n",
    "        print(\"Group {}, Epoch {}: G loss: {} vs. C loss: {}\".format(group_id, 1+i, g_loss, c_loss));\n",
    "        \n",
    "        # delete the history to free memory\n",
    "        del history, history2\n",
    "        \n",
    "        # make a new set of notes\n",
    "        res_noise = np.random.random((1, g_input_size));\n",
    "        res_input = conv_input(res_noise, extvar);\n",
    "        _resgenerated, res_map, _resclass = mmodel.predict(res_input);\n",
    "        if plot_map:\n",
    "            plot_current_map(tf.convert_to_tensor(res_map, dtype=tf.float32));\n",
    "        \n",
    "        # early return if found a good solution\n",
    "        # good is (inside the map boundary)\n",
    "        if i >= good_epoch:\n",
    "#             current_map = construct_map_with_sliders(tf.convert_to_tensor(res, dtype=\"float32\"), extvar=extvar);\n",
    "            current_map = res_map;\n",
    "            if inblock_trueness(current_map[:, :, 0:2]).numpy()[0] == 0 and inblock_trueness(current_map[:, :, 4:6]).numpy()[0] == 0:\n",
    "                # debugging options to check map integrity\n",
    "#                 print(tf.reduce_mean(current_map));\n",
    "#                 print(\"-----MAPLAYER-----\")\n",
    "#                 print(tf.reduce_mean(mapping_layer(conv_input(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar))));\n",
    "#                 print(\"-----CMWS-----\")\n",
    "#                 print(tf.reduce_mean(construct_map_with_sliders(tf.convert_to_tensor(_resgenerated, dtype=\"float32\"), extvar=mapping_layer.extvar)));\n",
    "                break;\n",
    "\n",
    "#     plot_history(history);\n",
    "#     plot_history(history2);\n",
    "    if plot_map:\n",
    "        for i in range(3): # from our testing, any random input generates nearly the same map\n",
    "            plot_noise = np.random.random((1, g_input_size));\n",
    "            plot_input = conv_input(plot_noise, extvar);\n",
    "            _plotgenerated, plot_mapped, _plotclass = mmodel.predict(plot_input);\n",
    "            plot_current_map(tf.convert_to_tensor(plot_mapped, dtype=tf.float32));\n",
    "    \n",
    "#     del mmodel, mapping_layer;\n",
    "    \n",
    "    return res_map.squeeze();\n",
    "#     onoise = np.random.random((1, g_input_size));\n",
    "    \n",
    "#     return construct_map_with_sliders(tf.convert_to_tensor(gmodel.predict(onoise)), extvar=extvar).numpy().squeeze();\n",
    "\n",
    "# generate the map (main function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_map():\n",
    "    o = [];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    pos = [np.random.randint(100, 412), np.random.randint(80, 304)];\n",
    "    models = make_models();\n",
    "    \n",
    "    print(\"# of groups: {}\".format(timestamps.shape[0] // note_group_size));\n",
    "    for i in range(timestamps.shape[0] // note_group_size):\n",
    "        z = generate_set(models, begin = i * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = i, plot_map=False) * np.array([512, 384, 1, 1, 512, 384]);\n",
    "        pos = z[-1, 0:2];\n",
    "        o.append(z);\n",
    "    a = np.concatenate(o, axis=0);\n",
    "    return a;\n",
    "\n",
    "# generate a test map (debugging function)\n",
    "# dist_multiplier in #6 is used here\n",
    "def generate_test():\n",
    "    o = [];\n",
    "    pos = [384, 288];\n",
    "    note_group_size = GAN_PARAMS[\"note_group_size\"];\n",
    "    generate_set(begin = 3 * note_group_size, start_pos = pos, length_multiplier = dist_multiplier, group_id = 3, plot_map=True);\n",
    "\n",
    "# for debugging only! it should be sent to node load_map.js c instead.\n",
    "def print_osu_text(a):\n",
    "    for i, ai in enumerate(a):\n",
    "        if not is_slider[i]:\n",
    "            print(\"{},{},{},1,0,0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i])));\n",
    "        else:\n",
    "            print(\"{},{},{},2,0,L|{}:{},1,{},0:0:0\".format(int(ai[0]), int(ai[1]), int(timestamps[i]), int(round(ai[0] + ai[2] * slider_lengths[i])), int(round(ai[1] + ai[3] * slider_lengths[i])), int(slider_length_base[i] * slider_ticks[i])));\n",
    "    \n",
    "osu_a = generate_map();\n",
    "# generate_test();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now convert the generated flow data to a dict, and mix it into the JSON file converted from .osu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this needs to be JSON serializable, so we need to carefully convert the types.\n",
    "# Numpy somehow got its own types like numpy.int64 that does not allow the data to be serialized to JSON.\n",
    "def convert_to_osu_obj(obj_array):\n",
    "    output = [];\n",
    "    for i, obj in enumerate(obj_array):\n",
    "        if not is_slider[i]: # is a circle; does not consider spinner for now.\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 1,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"index\": i\n",
    "            };\n",
    "        else:\n",
    "            obj_dict = {\n",
    "                \"x\": int(obj[0]),\n",
    "                \"y\": int(obj[1]),\n",
    "                \"type\": 2,\n",
    "                \"time\": int(timestamps[i]),\n",
    "                \"hitsounds\": 0,\n",
    "                \"extHitsounds\": \"0:0:0\",\n",
    "                \"sliderGenerator\": {\n",
    "                    \"type\": int(slider_types[i]),\n",
    "                    \"dOut\": [float(obj[2]), float(obj[3])],\n",
    "                    \"len\": float(slider_length_base[i] * slider_ticks[i]),\n",
    "                    \"ticks\": int(slider_ticks[i]),\n",
    "                    \"endpoint\": [int(obj[4]), int(obj[5])]\n",
    "                },\n",
    "                \"index\": i\n",
    "            };\n",
    "        output.append(obj_dict);\n",
    "    return output;\n",
    "\n",
    "def get_osu_file_name(metadata):\n",
    "    artist = metadata[\"artist\"];\n",
    "    title = metadata[\"title\"];\n",
    "    creator = metadata[\"creator\"];\n",
    "    diffname = metadata[\"diffname\"];\n",
    "    outname = (artist+\" - \" if len(artist) > 0 else \"\") + title + \" (\" + creator + \") [\" + diffname + \"].osu\";\n",
    "    outname = re.sub(\"[^a-zA-Z0-9\\(\\)\\[\\] \\.\\,\\!\\~\\`\\{\\}\\-\\_\\=\\+\\&\\^\\@\\#\\$\\%\\;\\']\",\"\", outname);\n",
    "    return outname;\n",
    "\n",
    "osu_obj_array = convert_to_osu_obj(osu_a);\n",
    "\n",
    "with open(\"mapthis.json\", encoding=\"utf-8\") as map_json:\n",
    "    map_dict = json.load(map_json);\n",
    "    map_meta = map_dict[\"meta\"];\n",
    "    filename = get_osu_file_name(map_meta);\n",
    "    map_dict[\"obj\"] = osu_obj_array;\n",
    "\n",
    "with open('mapthis.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "    json.dump(map_dict, outfile, ensure_ascii=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, ask node.js to convert the JSON back to a .osu file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success! finished on: 2019-06-29 12:48:58.047275\n"
     ]
    }
   ],
   "source": [
    "subprocess.call([\"node\", \"load_map.js\", \"c\", \"mapthis.json\", filename]);\n",
    "print(\"success! finished on: {}\".format(datetime.now()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vPnIVuaSJwWz"
   },
   "source": [
    "If it works alright, you should have a nice .osu file under the folder of these notebooks now!\n",
    "\n",
    "If it does not work, please tell me the problem so probably I could fix it!\n",
    "\n",
    "@2019/6/29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
